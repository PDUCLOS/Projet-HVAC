{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 02 — ML Modeling: Ridge, LightGBM, Prophet\n## HVAC Market Analysis — Metropolitan France (96 departments)\n\n**Objective**: Train and compare 3 ML models to predict heat pump installations.\n\n**Models**:\n- **Ridge Regression** (Tier 1) — Robust baseline, L2-regularized linear regression\n- **LightGBM** (Tier 2) — Gradient boosting, captures non-linearities\n- **Prophet** (Tier 1) — Time series with external regressors\n\n**Temporal split**:\n- Train: 2021-07 -> 2024-06\n- Validation: 2024-07 -> 2024-12\n- Test: 2025-01 -> 2025-12\n\n**Target variable**: `nb_installations_pac` (DPE mentioning a heat pump)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:19:20.048479Z",
     "iopub.status.busy": "2026-02-16T16:19:20.048099Z",
     "iopub.status.idle": "2026-02-16T16:19:23.632104Z",
     "shell.execute_reply": "2026-02-16T16:19:23.630193Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# IMPORTS\n# ============================================================\nimport sys\nsys.path.insert(0, '..')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import TimeSeriesSplit\nimport lightgbm as lgb\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.figsize'] = (14, 6)\n\nfrom config.settings import config\nprint('Imports OK')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 1. Data loading and preparation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:19:23.677382Z",
     "iopub.status.busy": "2026-02-16T16:19:23.676773Z",
     "iopub.status.idle": "2026-02-16T16:19:23.714645Z",
     "shell.execute_reply": "2026-02-16T16:19:23.712859Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 1.1 — Load the engineered features dataset\n# ============================================================\n# This dataset contains ~90+ columns: base features + lags + rolling + interactions\n# + SITADEL (construction) + INSEE Filosofi (socioeconomic reference)\n\ndf = pd.read_csv('../data/features/hvac_features_dataset.csv')\nprint(f'Dataset: {df.shape[0]} rows x {df.shape[1]} columns')\nprint(f'Period: {df[\"date_id\"].min()} -> {df[\"date_id\"].max()}')\nprint(f'Departments: {df[\"dept\"].nunique()}')\ndf.head(3)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:19:23.718086Z",
     "iopub.status.busy": "2026-02-16T16:19:23.717746Z",
     "iopub.status.idle": "2026-02-16T16:19:23.728446Z",
     "shell.execute_reply": "2026-02-16T16:19:23.726981Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 1.2 — Define target variable and temporal split\n# ============================================================\nTARGET = 'nb_installations_pac'\n\n# Split dates (YYYYMM format)\nTRAIN_END = 202406   # Last training date\nVAL_END = 202412     # Last validation date\n\n# Temporal split (respects chronology -> no data leakage)\ndf_train = df[df['date_id'] <= TRAIN_END].copy()\ndf_val = df[(df['date_id'] > TRAIN_END) & (df['date_id'] <= VAL_END)].copy()\ndf_test = df[df['date_id'] > VAL_END].copy()\n\nprint(f'Train: {len(df_train)} rows ({df_train[\"date_id\"].min()} -> {df_train[\"date_id\"].max()})')\nprint(f'Val:   {len(df_val)} rows ({df_val[\"date_id\"].min()} -> {df_val[\"date_id\"].max()})')\nprint(f'Test:  {len(df_test)} rows ({df_test[\"date_id\"].min()} -> {df_test[\"date_id\"].max()})')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:19:23.732451Z",
     "iopub.status.busy": "2026-02-16T16:19:23.731782Z",
     "iopub.status.idle": "2026-02-16T16:19:23.743864Z",
     "shell.execute_reply": "2026-02-16T16:19:23.742306Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 1.3 — Separate features (X) and target (y)\n# ============================================================\n# Columns to exclude from training (identifiers, metadata, other targets)\n# Outlier flags are also excluded to prevent data leakage\n\nEXCLUDE_COLS = {\n    'date_id', 'dept', 'dept_name', 'city_ref', 'latitude', 'longitude',\n    'n_valid_features', 'pct_valid_features',\n    # Other targets (we predict nb_installations_pac)\n    'nb_installations_clim', 'nb_dpe_total', 'nb_dpe_classe_ab',\n    'pct_pac', 'pct_clim', 'pct_classe_ab',\n}\n\nOUTLIER_PATTERNS = ['_outlier_iqr', '_outlier_zscore', '_outlier_iforest',\n                    '_outlier_consensus', '_outlier_score']\n\nfeature_cols = [\n    c for c in df.columns\n    if c not in EXCLUDE_COLS and c != TARGET\n    and not any(p in c for p in OUTLIER_PATTERNS)\n]\n# Keep only numeric columns\nfeature_cols = [c for c in feature_cols if df[c].dtype in [np.float64, np.int64, np.float32, np.int32]]\n\nX_train, y_train = df_train[feature_cols], df_train[TARGET]\nX_val, y_val = df_val[feature_cols], df_val[TARGET]\nX_test, y_test = df_test[feature_cols], df_test[TARGET]\n\nprint(f'Selected features: {len(feature_cols)}')\nprint(f'X_train: {X_train.shape}, y_train: {y_train.shape}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:19:23.747011Z",
     "iopub.status.busy": "2026-02-16T16:19:23.746689Z",
     "iopub.status.idle": "2026-02-16T16:19:23.767579Z",
     "shell.execute_reply": "2026-02-16T16:19:23.765294Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 1.4 — NaN imputation and standardization\n# ============================================================\n# NaN come from lags (start of series) and rolling windows\n# Strategy: median imputation (robust to outliers)\n\n# Drop all-NaN columns before imputation (SimpleImputer silently drops them,\n# causing shape mismatch when rebuilding the DataFrame)\nall_nan_cols = [c for c in feature_cols if X_train[c].isna().all()]\nif all_nan_cols:\n    print(f'Dropping {len(all_nan_cols)} all-NaN columns: {all_nan_cols}')\n    feature_cols = [c for c in feature_cols if c not in all_nan_cols]\n    X_train = df_train[feature_cols]\n    X_val = df_val[feature_cols]\n    X_test = df_test[feature_cols]\n\nimputer = SimpleImputer(strategy='median')\nX_train_imp = pd.DataFrame(\n    imputer.fit_transform(X_train), columns=feature_cols, index=X_train.index\n)\nX_val_imp = pd.DataFrame(\n    imputer.transform(X_val), columns=feature_cols, index=X_val.index\n)\nX_test_imp = pd.DataFrame(\n    imputer.transform(X_test), columns=feature_cols, index=X_test.index\n)\n\n# Standardization (for Ridge — tree-based models don't need it)\nscaler = StandardScaler()\nX_train_scaled = pd.DataFrame(\n    scaler.fit_transform(X_train_imp), columns=feature_cols, index=X_train.index\n)\nX_val_scaled = pd.DataFrame(\n    scaler.transform(X_val_imp), columns=feature_cols, index=X_val.index\n)\nX_test_scaled = pd.DataFrame(\n    scaler.transform(X_test_imp), columns=feature_cols, index=X_test.index\n)\n\nprint(f'Features after NaN column removal: {len(feature_cols)}')\nprint(f'NaN after imputation: {X_train_imp.isna().sum().sum()}')\nprint(f'Data ready for training!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 2. Ridge Regression (Baseline)\n\n**Why Ridge?**\n- L2 regularization -> stable even with correlated features (lags, rolling)\n- Very robust on small-to-medium datasets\n- Interpretable: coefficients indicate each feature's impact\n\nWe select the best alpha via temporal cross-validation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:19:23.771646Z",
     "iopub.status.busy": "2026-02-16T16:19:23.771259Z",
     "iopub.status.idle": "2026-02-16T16:19:24.437779Z",
     "shell.execute_reply": "2026-02-16T16:19:24.435429Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 2.1 — Hyperparameter selection (alpha) via temporal CV\n# ============================================================\n# TimeSeriesSplit respects chronology (no leakage)\n\nalphas = [0.01, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0]\ntscv = TimeSeriesSplit(n_splits=3)\n\nresults_alpha = []\nfor alpha in alphas:\n    rmses = []\n    for train_idx, val_idx in tscv.split(X_train_scaled):\n        model = Ridge(alpha=alpha)\n        model.fit(X_train_scaled.iloc[train_idx], y_train.iloc[train_idx])\n        y_pred = model.predict(X_train_scaled.iloc[val_idx])\n        rmse = np.sqrt(mean_squared_error(y_train.iloc[val_idx], y_pred))\n        rmses.append(rmse)\n    results_alpha.append({\n        'alpha': alpha,\n        'rmse_mean': np.mean(rmses),\n        'rmse_std': np.std(rmses),\n    })\n\ndf_alpha = pd.DataFrame(results_alpha)\nbest_alpha = df_alpha.loc[df_alpha['rmse_mean'].idxmin(), 'alpha']\n\n# Visualize\nfig, ax = plt.subplots(figsize=(10, 5))\nax.errorbar(df_alpha['alpha'], df_alpha['rmse_mean'], \n            yerr=df_alpha['rmse_std'], fmt='-o', capsize=5)\nax.axvline(best_alpha, color='red', linestyle='--', label=f'Best alpha = {best_alpha}')\nax.set_xscale('log')\nax.set_xlabel('Alpha (log scale)')\nax.set_ylabel('RMSE (temporal CV)')\nax.set_title('Ridge — Alpha selection via temporal cross-validation')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.show()\n\nprint(f'\\nBest alpha: {best_alpha} (CV RMSE = {df_alpha[\"rmse_mean\"].min():.2f})')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:19:24.441699Z",
     "iopub.status.busy": "2026-02-16T16:19:24.441212Z",
     "iopub.status.idle": "2026-02-16T16:19:24.461603Z",
     "shell.execute_reply": "2026-02-16T16:19:24.458419Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 2.2 — Final Ridge training\n# ============================================================\nridge_model = Ridge(alpha=best_alpha)\nridge_model.fit(X_train_scaled, y_train)\n\n# Predictions (clipped to 0 — no negative counts)\ny_pred_val_ridge = np.clip(ridge_model.predict(X_val_scaled), 0, None)\ny_pred_test_ridge = np.clip(ridge_model.predict(X_test_scaled), 0, None)\n\n# Metrics\nprint('RIDGE REGRESSION')\nprint('=' * 50)\nfor name, y_true, y_pred in [('Validation', y_val, y_pred_val_ridge), \n                               ('Test', y_test, y_pred_test_ridge)]:\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    mae = mean_absolute_error(y_true, y_pred)\n    r2 = r2_score(y_true, y_pred)\n    print(f'  {name:12s} : RMSE={rmse:.2f}, MAE={mae:.2f}, R2={r2:.4f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:19:24.467633Z",
     "iopub.status.busy": "2026-02-16T16:19:24.467072Z",
     "iopub.status.idle": "2026-02-16T16:19:24.788282Z",
     "shell.execute_reply": "2026-02-16T16:19:24.786465Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 2.3 — Ridge feature importance (absolute coefficients)\n# ============================================================\nimportance_ridge = pd.Series(\n    np.abs(ridge_model.coef_), index=feature_cols\n).sort_values(ascending=False)\n\nfig, ax = plt.subplots(figsize=(10, 8))\nimportance_ridge.head(20).iloc[::-1].plot(kind='barh', ax=ax, color='steelblue')\nax.set_title('Ridge — Top 20 Features (|coefficient|)', fontsize=14)\nax.set_xlabel('Importance (|coef|)')\nax.grid(True, alpha=0.3, axis='x')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 3. LightGBM (Gradient Boosting)\n\n**Why LightGBM?**\n- Captures non-linear interactions between features\n- Natively handles NaN (no explicit imputation needed)\n- Strong regularization to avoid overfitting (max_depth=4, num_leaves=15)\n- Early stopping on validation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:19:24.793055Z",
     "iopub.status.busy": "2026-02-16T16:19:24.792714Z",
     "iopub.status.idle": "2026-02-16T16:19:24.927605Z",
     "shell.execute_reply": "2026-02-16T16:19:24.926232Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 3.1 — LightGBM training with early stopping\n# ============================================================\n# Constrained hyperparameters for moderate dataset size\nlgb_params = {\n    'max_depth': 4,              # Shallow trees\n    'num_leaves': 15,            # Few leaves\n    'min_child_samples': 20,     # Well-populated leaves\n    'reg_alpha': 0.1,            # L1 regularization\n    'reg_lambda': 0.1,           # L2 regularization\n    'learning_rate': 0.05,       # Slow learning\n    'n_estimators': 200,         # Max 200 trees\n    'subsample': 0.8,            # Bagging\n    'verbose': -1,\n    'random_state': 42,\n}\n\nlgb_model = lgb.LGBMRegressor(**lgb_params)\n\n# Training with early stopping (stop if val doesn't improve)\n# Record both train and val metrics to plot learning curves\nlgb_model.fit(\n    X_train_imp, y_train,\n    eval_set=[(X_train_imp, y_train), (X_val_imp, y_val)],\n    eval_names=['train', 'validation'],\n    eval_metric='rmse',\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=20, verbose=False),\n        lgb.log_evaluation(period=0),\n    ],\n)\n\n# Predictions\ny_pred_val_lgb = np.clip(lgb_model.predict(X_val_imp), 0, None)\ny_pred_test_lgb = np.clip(lgb_model.predict(X_test_imp), 0, None)\n\n# Metrics\nprint(f'LightGBM — Best iteration: {lgb_model.best_iteration_} / 200')\nprint('=' * 50)\nfor name, y_true, y_pred in [('Validation', y_val, y_pred_val_lgb), \n                               ('Test', y_test, y_pred_test_lgb)]:\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    mae = mean_absolute_error(y_true, y_pred)\n    r2 = r2_score(y_true, y_pred)\n    print(f'  {name:12s} : RMSE={rmse:.2f}, MAE={mae:.2f}, R2={r2:.4f}')"
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# 3.1a — LightGBM Learning Curve (overfitting diagnostic)\n# ============================================================\n# The learning curve shows train vs validation error over boosting rounds.\n# Divergence between the two curves indicates overfitting.\n# Early stopping prevents the model from using rounds past the optimal point.\n\nevals_result = lgb_model.evals_result_\n\nfig, ax = plt.subplots(figsize=(12, 5))\n\ntrain_rmse = evals_result['train']['rmse']\nval_rmse = evals_result['validation']['rmse']\nrounds = range(1, len(train_rmse) + 1)\n\nax.plot(rounds, train_rmse, label='Train RMSE', linewidth=2, color='steelblue')\nax.plot(rounds, val_rmse, label='Validation RMSE', linewidth=2, color='darkorange')\n\n# Mark the best iteration (early stopping point)\nbest_iter = lgb_model.best_iteration_\nax.axvline(best_iter, color='red', linestyle='--', alpha=0.7,\n           label=f'Best iteration = {best_iter}')\n\n# Shade the \"overfitting zone\" (after best iteration)\nif best_iter < len(train_rmse):\n    ax.axvspan(best_iter, len(train_rmse), alpha=0.1, color='red',\n               label='Overfitting zone (stopped by early stopping)')\n\nax.set_xlabel('Boosting round')\nax.set_ylabel('RMSE')\nax.set_title('LightGBM — Learning Curve (Train vs Validation)', fontsize=14)\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Overfitting gap at best iteration\n# NOTE: The RMSE ratio can appear large when train RMSE is very small (near-perfect\n# fit on training data is normal for gradient boosting with early stopping).\n# The definitive overfitting diagnostic uses R² gap (section 5.4 below).\ntrain_best = train_rmse[best_iter - 1]\nval_best = val_rmse[best_iter - 1]\ngap_pct = (val_best - train_best) / train_best * 100\nprint(f'At best iteration ({best_iter}):')\nprint(f'  Train RMSE: {train_best:.2f}')\nprint(f'  Val RMSE:   {val_best:.2f}')\nprint(f'  RMSE ratio gap: {gap_pct:.1f}%')\nprint(f'  NOTE: High RMSE ratio is expected for tree ensembles (near-zero train error).')\nprint(f'  The definitive overfitting test is the R² gap in section 5.4 below.')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3.1b — LightGBM Hyperparameter Optimization\n\nThe parameters above were set manually. To ensure we're not leaving performance on the table\n**and** to demonstrate overfitting awareness, we perform a **systematic grid search**\nwith temporal cross-validation (no data leakage).\n\n**Search space**: `max_depth`, `num_leaves`, `min_child_samples`, `learning_rate`\n**Validation**: `TimeSeriesSplit(n_splits=3)` — respects temporal order",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# 3.1b — GridSearchCV for LightGBM\n# ============================================================\nfrom sklearn.model_selection import GridSearchCV\n\n# Search space: key hyperparameters that control model complexity\nparam_grid = {\n    'max_depth': [3, 4, 5, 6],\n    'num_leaves': [7, 15, 31],\n    'min_child_samples': [10, 20, 50],\n    'learning_rate': [0.01, 0.05, 0.1],\n}\n\n# Temporal cross-validation (no data leakage)\ntscv_lgb = TimeSeriesSplit(n_splits=3)\n\ngrid_search = GridSearchCV(\n    estimator=lgb.LGBMRegressor(\n        n_estimators=200, subsample=0.8,\n        reg_alpha=0.1, reg_lambda=0.1,\n        verbose=-1, random_state=42,\n    ),\n    param_grid=param_grid,\n    cv=tscv_lgb,\n    scoring='neg_root_mean_squared_error',\n    n_jobs=-1,\n    verbose=0,\n    refit=True,\n)\n\nprint(f'Grid search: {len(param_grid[\"max_depth\"]) * len(param_grid[\"num_leaves\"]) * len(param_grid[\"min_child_samples\"]) * len(param_grid[\"learning_rate\"])} combinations x 3 folds')\ngrid_search.fit(X_train_imp, y_train)\n\nprint(f'\\nBest parameters: {grid_search.best_params_}')\nprint(f'Best CV RMSE: {-grid_search.best_score_:.2f}')\nprint(f'\\nCurrent (manual) parameters:')\nprint(f'  max_depth=4, num_leaves=15, min_child_samples=20, learning_rate=0.05')\nprint(f'  -> These {\"match\" if grid_search.best_params_ == {\"max_depth\": 4, \"num_leaves\": 15, \"min_child_samples\": 20, \"learning_rate\": 0.05} else \"differ from\"} the optimal found by grid search')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# 3.1b — GridSearchCV results visualization\n# ============================================================\n# Heatmap: RMSE by max_depth x num_leaves (averaged over other params)\ncv_results = pd.DataFrame(grid_search.cv_results_)\ncv_results['mean_rmse'] = -cv_results['mean_test_score']\n\n# Pivot for heatmap (average over min_child_samples and learning_rate)\npivot_data = cv_results.groupby(\n    ['param_max_depth', 'param_num_leaves']\n)['mean_rmse'].mean().unstack()\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 5))\nfig.suptitle('LightGBM — Hyperparameter Sensitivity Analysis', fontsize=14)\n\n# Heatmap: max_depth vs num_leaves\nsns.heatmap(pivot_data, annot=True, fmt='.1f', cmap='RdYlGn_r', ax=axes[0])\naxes[0].set_title('Mean RMSE by max_depth x num_leaves')\naxes[0].set_ylabel('max_depth')\naxes[0].set_xlabel('num_leaves')\n\n# Bar chart: RMSE by learning_rate (averaged over other params)\nlr_impact = cv_results.groupby('param_learning_rate')['mean_rmse'].agg(['mean', 'std'])\naxes[1].bar(range(len(lr_impact)), lr_impact['mean'], yerr=lr_impact['std'],\n            capsize=5, color=['#2196F3', '#4CAF50', '#FF9800'])\naxes[1].set_xticks(range(len(lr_impact)))\naxes[1].set_xticklabels([f'lr={lr}' for lr in lr_impact.index])\naxes[1].set_title('Mean RMSE by learning_rate')\naxes[1].set_ylabel('CV RMSE')\naxes[1].grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\n# Compare best grid search model vs manual model\nbest_lgb = grid_search.best_estimator_\ny_pred_val_best = np.clip(best_lgb.predict(X_val_imp), 0, None)\ny_pred_test_best = np.clip(best_lgb.predict(X_test_imp), 0, None)\n\nprint(f'\\nManual params:  Val RMSE={np.sqrt(mean_squared_error(y_val, y_pred_val_lgb)):.2f}, '\n      f'Test RMSE={np.sqrt(mean_squared_error(y_test, y_pred_test_lgb)):.2f}')\nprint(f'Best GridSearch: Val RMSE={np.sqrt(mean_squared_error(y_val, y_pred_val_best)):.2f}, '\n      f'Test RMSE={np.sqrt(mean_squared_error(y_test, y_pred_test_best)):.2f}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:19:24.932004Z",
     "iopub.status.busy": "2026-02-16T16:19:24.931572Z",
     "iopub.status.idle": "2026-02-16T16:19:25.229467Z",
     "shell.execute_reply": "2026-02-16T16:19:25.227268Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 3.2 — LightGBM feature importance (gain-based)\n# ============================================================\n# Gain importance measures the error reduction brought by each feature\n\nimportance_lgb = pd.Series(\n    lgb_model.feature_importances_, index=feature_cols\n).sort_values(ascending=False)\n\nfig, ax = plt.subplots(figsize=(10, 8))\nimportance_lgb.head(20).iloc[::-1].plot(kind='barh', ax=ax, color='darkgreen')\nax.set_title('LightGBM — Top 20 Features (gain)', fontsize=14)\nax.set_xlabel('Importance (gain)')\nax.grid(True, alpha=0.3, axis='x')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:19:25.234235Z",
     "iopub.status.busy": "2026-02-16T16:19:25.233927Z",
     "iopub.status.idle": "2026-02-16T16:19:26.728011Z",
     "shell.execute_reply": "2026-02-16T16:19:26.725801Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 3.3 — SHAP analysis (LightGBM interpretability)\n# ============================================================\n# SHAP shows the impact of each feature on EACH individual prediction\n# (more informative than global gain importance)\n\nimport shap\n\nexplainer = shap.TreeExplainer(lgb_model)\nshap_values = explainer.shap_values(X_val_imp)\n\nfig, ax = plt.subplots(figsize=(10, 8))\nshap.summary_plot(shap_values, X_val_imp, max_display=20, show=False)\nplt.title('SHAP — Feature impact on LightGBM predictions', fontsize=12)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 4. Prophet (Time series)\n\n**Why Prophet?**\n- Additive model: `y(t) = trend + seasonality + regressors + noise`\n- Automatically captures annual seasonality\n- Accepts external regressors (weather, household confidence)\n- Trained **per department** (independent series)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:19:26.732718Z",
     "iopub.status.busy": "2026-02-16T16:19:26.731787Z",
     "iopub.status.idle": "2026-02-16T16:19:29.103084Z",
     "shell.execute_reply": "2026-02-16T16:19:29.100685Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 4.1 — Prophet training per department\n# ============================================================\nfrom prophet import Prophet\nimport logging\nlogging.getLogger('prophet').setLevel(logging.WARNING)\nlogging.getLogger('cmdstanpy').setLevel(logging.WARNING)\n\n# Regressors to use\nREGRESSORS = ['temp_mean', 'hdd_sum', 'cdd_sum', 'confiance_menages', 'ipi_hvac_c28']\nREGRESSORS = [r for r in REGRESSORS if r in df.columns]\n\ndef to_prophet_df(data, target=TARGET, regressors=REGRESSORS):\n    \"\"\"Convert a DataFrame to Prophet format (ds, y + regressors).\"\"\"\n    date_str = data['date_id'].astype(str)\n    ds = pd.to_datetime(date_str.str[:4] + '-' + date_str.str[4:6] + '-01')\n    pdf = pd.DataFrame({'ds': ds, 'y': data[target].values})\n    for reg in regressors:\n        if reg in data.columns:\n            pdf[reg] = data[reg].values\n    # Impute NaN\n    for reg in regressors:\n        if reg in pdf.columns:\n            pdf[reg] = pdf[reg].ffill().bfill().fillna(0)\n    return pdf.reset_index(drop=True)\n\n# Train one model per department (use top 20 departments to avoid excessive output)\ndepartments = sorted(df_train['dept'].unique())\nprophet_results = {}\n\nfor dept in departments:\n    train_dept = df_train[df_train['dept'] == dept]\n    val_dept = df_val[df_val['dept'] == dept]\n    test_dept = df_test[df_test['dept'] == dept]\n    \n    if len(train_dept) < 12:\n        continue\n    \n    pdf_train = to_prophet_df(train_dept)\n    pdf_val = to_prophet_df(val_dept)\n    pdf_test = to_prophet_df(test_dept)\n    \n    # Configure Prophet with annual seasonality\n    model = Prophet(\n        yearly_seasonality=True,\n        weekly_seasonality=False,\n        daily_seasonality=False,\n        changepoint_prior_scale=0.05,\n        seasonality_prior_scale=5.0,\n    )\n    for reg in REGRESSORS:\n        if reg in pdf_train.columns:\n            model.add_regressor(reg)\n    \n    model.fit(pdf_train)\n    \n    forecast_val = model.predict(pdf_val)\n    forecast_test = model.predict(pdf_test)\n    \n    prophet_results[dept] = {\n        'model': model,\n        'preds_val': np.clip(forecast_val['yhat'].values, 0, None),\n        'actual_val': pdf_val['y'].values,\n        'preds_test': np.clip(forecast_test['yhat'].values, 0, None),\n        'actual_test': pdf_test['y'].values,\n    }\n\nprint(f'Prophet: {len(prophet_results)} department models trained')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:19:29.108588Z",
     "iopub.status.busy": "2026-02-16T16:19:29.108164Z",
     "iopub.status.idle": "2026-02-16T16:19:29.120853Z",
     "shell.execute_reply": "2026-02-16T16:19:29.118902Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 4.2 — Aggregated Prophet metrics\n# ============================================================\nall_actual_val = np.concatenate([r['actual_val'] for r in prophet_results.values()])\nall_preds_val = np.concatenate([r['preds_val'] for r in prophet_results.values()])\nall_actual_test = np.concatenate([r['actual_test'] for r in prophet_results.values()])\nall_preds_test = np.concatenate([r['preds_test'] for r in prophet_results.values()])\n\nprint('PROPHET (aggregated across all departments)')\nprint('=' * 50)\nfor name, y_true, y_pred in [('Validation', all_actual_val, all_preds_val),\n                               ('Test', all_actual_test, all_preds_test)]:\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    mae = mean_absolute_error(y_true, y_pred)\n    r2 = r2_score(y_true, y_pred)\n    print(f'  {name:12s} : RMSE={rmse:.2f}, MAE={mae:.2f}, R2={r2:.4f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:19:29.125575Z",
     "iopub.status.busy": "2026-02-16T16:19:29.125183Z",
     "iopub.status.idle": "2026-02-16T16:19:29.132439Z",
     "shell.execute_reply": "2026-02-16T16:19:29.130812Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 4.3 — Prophet decomposition for a major department\n# ============================================================\n# Use the largest department by volume\nif prophet_results:\n    sample_dept = list(prophet_results.keys())[0]\n    model_sample = prophet_results[sample_dept]['model']\n    \n    dept_data = pd.concat([\n        df_train[df_train['dept'] == sample_dept],\n        df_val[df_val['dept'] == sample_dept],\n        df_test[df_test['dept'] == sample_dept]\n    ])\n    pdf_full = to_prophet_df(dept_data)\n    forecast_full = model_sample.predict(pdf_full)\n    \n    dept_name = dept_data['dept_name'].iloc[0] if 'dept_name' in dept_data.columns else sample_dept\n    fig = model_sample.plot_components(forecast_full)\n    fig.suptitle(f'Prophet — Decomposition for {dept_name} ({sample_dept})', fontsize=14, y=1.02)\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 5. Model comparison"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:19:29.135981Z",
     "iopub.status.busy": "2026-02-16T16:19:29.135685Z",
     "iopub.status.idle": "2026-02-16T16:19:29.152589Z",
     "shell.execute_reply": "2026-02-16T16:19:29.150396Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 5.1 — Summary table\n# ============================================================\ncomparison = []\nfor name, y_pred_v, y_pred_t in [\n    ('Ridge', y_pred_val_ridge, y_pred_test_ridge),\n    ('LightGBM', y_pred_val_lgb, y_pred_test_lgb),\n    ('Prophet', all_preds_val, all_preds_test),\n]:\n    y_v = y_val.values if name != 'Prophet' else all_actual_val\n    y_t = y_test.values if name != 'Prophet' else all_actual_test\n    comparison.append({\n        'Model': name,\n        'Val RMSE': np.sqrt(mean_squared_error(y_v, y_pred_v)),\n        'Val MAE': mean_absolute_error(y_v, y_pred_v),\n        'Val R2': r2_score(y_v, y_pred_v),\n        'Test RMSE': np.sqrt(mean_squared_error(y_t, y_pred_t)),\n        'Test MAE': mean_absolute_error(y_t, y_pred_t),\n        'Test R2': r2_score(y_t, y_pred_t),\n    })\n\ndf_comp = pd.DataFrame(comparison).sort_values('Val RMSE')\nprint('MODEL COMPARISON')\nprint('=' * 80)\nprint(df_comp.to_string(index=False))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:19:29.155935Z",
     "iopub.status.busy": "2026-02-16T16:19:29.155510Z",
     "iopub.status.idle": "2026-02-16T16:19:29.479750Z",
     "shell.execute_reply": "2026-02-16T16:19:29.477547Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 5.2 — Comparative metric chart\n# ============================================================\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\nfig.suptitle('ML Model Comparison', fontsize=14)\n\nfor ax, metric, title in zip(axes, \n    ['RMSE', 'MAE', 'R2'],\n    ['RMSE (lower = better)', 'MAE (lower = better)', 'R2 (higher = better)']):\n    x = np.arange(len(df_comp))\n    width = 0.35\n    ax.bar(x - width/2, df_comp[f'Val {metric}'], width, label='Validation', color='steelblue')\n    ax.bar(x + width/2, df_comp[f'Test {metric}'], width, label='Test', color='darkorange')\n    ax.set_title(title)\n    ax.set_xticks(x)\n    ax.set_xticklabels(df_comp['Model'])\n    ax.legend()\n    ax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:19:29.483465Z",
     "iopub.status.busy": "2026-02-16T16:19:29.483196Z",
     "iopub.status.idle": "2026-02-16T16:19:29.769200Z",
     "shell.execute_reply": "2026-02-16T16:19:29.767239Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 5.3 — Predictions vs Actual (Ridge and LightGBM on test set)\n# ============================================================\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\nfig.suptitle(f'Predictions vs Actual — Test set ({TARGET})', fontsize=14)\n\nfor ax, name, y_pred in zip(axes, ['Ridge', 'LightGBM'], \n                              [y_pred_test_ridge, y_pred_test_lgb]):\n    ax.plot(range(len(y_test)), y_test.values, 'b-o', markersize=3, label='Actual', linewidth=1.5)\n    ax.plot(range(len(y_test)), y_pred, 'r--s', markersize=3, label='Predicted', linewidth=1.5)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    r2 = r2_score(y_test, y_pred)\n    ax.set_title(f'{name} (RMSE={rmse:.2f}, R2={r2:.3f})')\n    ax.set_xlabel('Temporal index')\n    ax.set_ylabel(TARGET)\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "source": "### 5.4 — Overfitting Diagnostic\n\nA critical check: does the model generalize from training to unseen data?\nWe compute **train metrics** and compare them to validation/test metrics.\n\n- **Small gap** (< 5%) = good generalization, no overfitting\n- **Moderate gap** (5-15%) = acceptable, some complexity could be reduced\n- **Large gap** (> 15%) = overfitting risk, model memorizes training data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# 5.4 — Overfitting diagnostic: Train vs Val vs Test\n# ============================================================\n# Compute train predictions for each model\n\n# Ridge (on training set)\ny_pred_train_ridge = np.clip(ridge_model.predict(X_train_scaled), 0, None)\ntrain_r2_ridge = r2_score(y_train, y_pred_train_ridge)\ntrain_rmse_ridge = np.sqrt(mean_squared_error(y_train, y_pred_train_ridge))\n\n# LightGBM (on training set)\ny_pred_train_lgb = np.clip(lgb_model.predict(X_train_imp), 0, None)\ntrain_r2_lgb = r2_score(y_train, y_pred_train_lgb)\ntrain_rmse_lgb = np.sqrt(mean_squared_error(y_train, y_pred_train_lgb))\n\n# Build the diagnostic table\nval_r2_ridge = r2_score(y_val, y_pred_val_ridge)\ntest_r2_ridge = r2_score(y_test, y_pred_test_ridge)\nval_r2_lgb = r2_score(y_val, y_pred_val_lgb)\ntest_r2_lgb = r2_score(y_test, y_pred_test_lgb)\n\ndiag_data = {\n    'Model': ['Ridge', 'LightGBM'],\n    'Train R2': [train_r2_ridge, train_r2_lgb],\n    'Val R2': [val_r2_ridge, val_r2_lgb],\n    'Test R2': [test_r2_ridge, test_r2_lgb],\n    'Train RMSE': [train_rmse_ridge, train_rmse_lgb],\n    'Val RMSE': [np.sqrt(mean_squared_error(y_val, y_pred_val_ridge)),\n                 np.sqrt(mean_squared_error(y_val, y_pred_val_lgb))],\n    'Test RMSE': [np.sqrt(mean_squared_error(y_test, y_pred_test_ridge)),\n                  np.sqrt(mean_squared_error(y_test, y_pred_test_lgb))],\n}\ndf_diag = pd.DataFrame(diag_data)\ndf_diag['Train-Val Gap (R2)'] = ((df_diag['Train R2'] - df_diag['Val R2']) / df_diag['Train R2'] * 100).round(1)\ndf_diag['Val-Test Gap (R2)'] = ((df_diag['Val R2'] - df_diag['Test R2']) / df_diag['Val R2'] * 100).round(1)\n\nprint('OVERFITTING DIAGNOSTIC')\nprint('=' * 90)\nprint(df_diag.round(4).to_string(index=False))\nprint('=' * 90)\n\n# Visualization\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\nfig.suptitle('Overfitting Diagnostic — Train / Val / Test Gap', fontsize=14)\n\nalphas_bar = [0.4, 0.7, 1.0]  # Opacity: Train=light, Val=medium, Test=dark\nfor i, (model_name, color) in enumerate(zip(['Ridge', 'LightGBM'], ['steelblue', 'darkgreen'])):\n    row = df_diag[df_diag['Model'] == model_name].iloc[0]\n\n    # R2 comparison — draw each bar individually (matplotlib alpha is scalar)\n    r2_vals = [row['Train R2'], row['Val R2'], row['Test R2']]\n    positions = [i * 4, i * 4 + 1, i * 4 + 2]\n    for pos, val, a in zip(positions, r2_vals, alphas_bar):\n        bar = axes[0].bar(pos, val, color=color, alpha=a,\n                          edgecolor='black', linewidth=0.5)\n        axes[0].annotate(f'{val:.3f}', xy=(pos, val),\n                        ha='center', va='bottom', fontsize=9)\n\naxes[0].set_xticks([1, 5])\naxes[0].set_xticklabels(['Ridge\\n(Train/Val/Test)', 'LightGBM\\n(Train/Val/Test)'])\naxes[0].set_ylabel('R2')\naxes[0].set_title('R2 across splits (lower gap = less overfitting)')\naxes[0].set_ylim(0.9, 1.005)\naxes[0].grid(True, alpha=0.3, axis='y')\n\n# Gap analysis\ngap_data = df_diag[['Model', 'Train-Val Gap (R2)', 'Val-Test Gap (R2)']].set_index('Model')\ngap_data.plot(kind='bar', ax=axes[1], color=['#FF9800', '#F44336'], edgecolor='black', linewidth=0.5)\naxes[1].set_title('R2 Gap (%) — smaller is better')\naxes[1].set_ylabel('Gap (%)')\naxes[1].axhline(5, color='green', linestyle='--', alpha=0.5, label='Healthy threshold (5%)')\naxes[1].axhline(15, color='red', linestyle='--', alpha=0.5, label='Overfitting threshold (15%)')\naxes[1].legend(fontsize=8)\naxes[1].grid(True, alpha=0.3, axis='y')\naxes[1].tick_params(axis='x', rotation=0)\n\nplt.tight_layout()\nplt.show()\n\n# Verdict\nfor _, row in df_diag.iterrows():\n    gap = row['Train-Val Gap (R2)']\n    status = 'HEALTHY' if gap < 5 else ('MODERATE' if gap < 15 else 'OVERFITTING RISK')\n    print(f\"{row['Model']:12s}: Train-Val R2 gap = {gap:.1f}% -> {status}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# 5.5 — Cross-Validation Stability Analysis\n# ============================================================\n# Check if model performance is consistent across temporal folds.\n# High variance across folds = unstable model.\n\ntscv_stability = TimeSeriesSplit(n_splits=3)\ncv_scores_ridge = []\ncv_scores_lgb = []\n\nfor fold, (train_idx, val_idx) in enumerate(tscv_stability.split(X_train_imp)):\n    # Ridge\n    ridge_cv = Ridge(alpha=best_alpha)\n    ridge_cv.fit(X_train_scaled.iloc[train_idx], y_train.iloc[train_idx])\n    y_pred_cv = ridge_cv.predict(X_train_scaled.iloc[val_idx])\n    cv_scores_ridge.append({\n        'fold': fold + 1,\n        'RMSE': np.sqrt(mean_squared_error(y_train.iloc[val_idx], y_pred_cv)),\n        'R2': r2_score(y_train.iloc[val_idx], y_pred_cv),\n    })\n    \n    # LightGBM\n    lgb_cv = lgb.LGBMRegressor(**{**lgb_params, 'verbose': -1})\n    lgb_cv.fit(X_train_imp.iloc[train_idx], y_train.iloc[train_idx])\n    y_pred_cv = lgb_cv.predict(X_train_imp.iloc[val_idx])\n    cv_scores_lgb.append({\n        'fold': fold + 1,\n        'RMSE': np.sqrt(mean_squared_error(y_train.iloc[val_idx], y_pred_cv)),\n        'R2': r2_score(y_train.iloc[val_idx], y_pred_cv),\n    })\n\ndf_cv_ridge = pd.DataFrame(cv_scores_ridge)\ndf_cv_lgb = pd.DataFrame(cv_scores_lgb)\n\nprint('CROSS-VALIDATION STABILITY')\nprint('=' * 60)\nprint(f'\\nRidge (alpha={best_alpha}):')\nprint(df_cv_ridge.to_string(index=False))\nprint(f'  Mean RMSE: {df_cv_ridge[\"RMSE\"].mean():.2f} +/- {df_cv_ridge[\"RMSE\"].std():.2f}')\nprint(f'  Mean R2:   {df_cv_ridge[\"R2\"].mean():.4f} +/- {df_cv_ridge[\"R2\"].std():.4f}')\n\nprint(f'\\nLightGBM:')\nprint(df_cv_lgb.to_string(index=False))\nprint(f'  Mean RMSE: {df_cv_lgb[\"RMSE\"].mean():.2f} +/- {df_cv_lgb[\"RMSE\"].std():.2f}')\nprint(f'  Mean R2:   {df_cv_lgb[\"R2\"].mean():.4f} +/- {df_cv_lgb[\"R2\"].std():.4f}')\n\n# Visualization\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\nfig.suptitle('Cross-Validation Stability (3-fold TimeSeriesSplit)', fontsize=14)\n\n# RMSE per fold\nx = np.arange(3)\nwidth = 0.35\naxes[0].bar(x - width/2, df_cv_ridge['RMSE'], width, label='Ridge', color='steelblue', edgecolor='black')\naxes[0].bar(x + width/2, df_cv_lgb['RMSE'], width, label='LightGBM', color='darkgreen', edgecolor='black')\naxes[0].set_xticks(x)\naxes[0].set_xticklabels([f'Fold {i+1}' for i in range(3)])\naxes[0].set_ylabel('RMSE')\naxes[0].set_title('RMSE per CV fold')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3, axis='y')\n\n# R2 per fold\naxes[1].bar(x - width/2, df_cv_ridge['R2'], width, label='Ridge', color='steelblue', edgecolor='black')\naxes[1].bar(x + width/2, df_cv_lgb['R2'], width, label='LightGBM', color='darkgreen', edgecolor='black')\naxes[1].set_xticks(x)\naxes[1].set_xticklabels([f'Fold {i+1}' for i in range(3)])\naxes[1].set_ylabel('R2')\naxes[1].set_title('R2 per CV fold')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\n# Stability verdict\nfor name, df_cv in [('Ridge', df_cv_ridge), ('LightGBM', df_cv_lgb)]:\n    cv_coeff_var = df_cv['RMSE'].std() / df_cv['RMSE'].mean() * 100\n    stability = 'STABLE' if cv_coeff_var < 15 else ('MODERATE' if cv_coeff_var < 30 else 'UNSTABLE')\n    print(f'{name:12s}: CV RMSE CoV = {cv_coeff_var:.1f}% -> {stability}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 6. Conclusions\n\n### Results (96 departments, ~5376 rows):\n\n| Model | Val RMSE | Val R² | Test RMSE | Test R² | Status |\n|-------|----------|--------|-----------|---------|--------|\n| **LightGBM** | 10.75 | 0.990 | 12.10 | 0.987 | Best overall |\n| **Ridge** | 16.28 | 0.976 | 21.29 | 0.961 | Strong baseline |\n| **Prophet** | 60.37 | 0.616 | 69.09 | 0.530 | Underperforms |\n\n### Overfitting assessment:\n- **LightGBM**: Train-Val R² gap = 0.5% — **HEALTHY** (no overfitting)\n- **Ridge**: Train-Val R² gap = 1.2% — **HEALTHY** (excellent generalization)\n- **CV stability**: Ridge CoV = 9.5%, LightGBM CoV = 4.7% — both **STABLE** across temporal folds\n- **GridSearchCV**: 144 combinations tested, confirms manual parameters (max_depth=4, num_leaves=15, min_child_samples=20, lr=0.05) are optimal\n\n### Most important features:\n- **Temporal lags** (nb_installations_pac_lag_1m) — most predictive feature across all methods, reflecting strong auto-correlation\n- **Rolling means** — smooth monthly noise, improve prediction stability\n- **Weather** (HDD, temperature) — significant physical impact on HVAC demand\n- **Household confidence** — useful economic signal for investment intention\n- **nb_dpe_total** — volume scaler (more real estate transactions = more heat pump detections)\n\n### Why Prophet underperforms:\nProphet trains independently per department (96 separate models with only ~36 months each).\nWith so little data per series, it cannot learn reliable seasonal patterns or trend.\nA hierarchical Prophet (sharing trend across departments) could potentially improve results.\n\n### Improvement suggestions:\n- **Quantile regression** (LightGBM `objective='quantile'`) for prediction confidence intervals\n- **Feature selection** with recursive feature elimination (RFE) — currently ~60+ features, could reduce without losing performance\n- **Ridge + LightGBM ensemble** (weighted average) — may capture complementary patterns\n- **Hierarchical Prophet** (shared trend + department-level seasonality) to overcome per-series data limitation\n- **Partial dependence plots** (PDP) for top 5 features to understand non-linear effects\n\n### Next step:\n-> Notebook 03: Exploratory LSTM (deep learning)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 04 — Results and Model Comparison\n## HVAC Market Analysis — Metropolitan France (96 departments)\n\n**Objective**: Synthesize and analyze the results of all trained models.\n\n**Plan**:\n1. Load results\n2. Final comparison table\n3. Prediction visualizations\n4. Feature importance (SHAP)\n5. Error analysis\n6. Final recommendations"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:21:16.134649Z",
     "iopub.status.busy": "2026-02-16T16:21:16.134367Z",
     "iopub.status.idle": "2026-02-16T16:21:19.688920Z",
     "shell.execute_reply": "2026-02-16T16:21:19.686529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# IMPORTS\n",
    "# ============================================================\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "from config.settings import config\n",
    "print('Imports OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 1. Load training results\n\nResults are saved by `python -m src.pipeline train` in `data/models/`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:21:19.737083Z",
     "iopub.status.busy": "2026-02-16T16:21:19.736569Z",
     "iopub.status.idle": "2026-02-16T16:21:19.759700Z",
     "shell.execute_reply": "2026-02-16T16:21:19.758278Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 1.1 — Load the CSV summary\n# ============================================================\nresults_path = Path('../data/models/training_results.csv')\nif results_path.exists():\n    df_results = pd.read_csv(results_path)\n    print(f'Results loaded: {len(df_results)} models')\n    display(df_results)\nelse:\n    print('No results found. Run: python -m src.pipeline train')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:21:19.762914Z",
     "iopub.status.busy": "2026-02-16T16:21:19.762601Z",
     "iopub.status.idle": "2026-02-16T16:21:19.787490Z",
     "shell.execute_reply": "2026-02-16T16:21:19.785877Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 1.2 — Load dataset and recreate splits\n# ============================================================\nTARGET = 'nb_installations_pac'\nTRAIN_END = 202406\nVAL_END = 202412\n\ndf = pd.read_csv('../data/features/hvac_features_dataset.csv')\n\n# Splits\ndf_train = df[df['date_id'] <= TRAIN_END].copy()\ndf_val = df[(df['date_id'] > TRAIN_END) & (df['date_id'] <= VAL_END)].copy()\ndf_test = df[df['date_id'] > VAL_END].copy()\n\n# Time axis\ndf['date'] = pd.to_datetime(\n    df['date_id'].astype(str).str[:4] + '-' + \n    df['date_id'].astype(str).str[4:] + '-01'\n)\n\nprint(f'Train: {len(df_train)} | Val: {len(df_val)} | Test: {len(df_test)}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:21:19.791881Z",
     "iopub.status.busy": "2026-02-16T16:21:19.791580Z",
     "iopub.status.idle": "2026-02-16T16:21:20.048098Z",
     "shell.execute_reply": "2026-02-16T16:21:20.046916Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 1.3 — Load saved models and re-predict\n# ============================================================\n# Use the same EXCLUDE_COLS as train.py so features match saved models\n# Outlier flags are excluded to prevent data leakage\n\nmodels_dir = Path('../data/models')\n\nEXCLUDE_COLS = {\n    'date_id', 'dept', 'dept_name', 'city_ref', 'latitude', 'longitude',\n    'n_valid_features', 'pct_valid_features',\n}\nOTHER_TARGETS = {\n    'nb_installations_clim', 'nb_dpe_total', 'nb_dpe_classe_ab',\n}\nOUTLIER_PATTERNS = ['_outlier_iqr', '_outlier_zscore', '_outlier_iforest',\n                    '_outlier_consensus', '_outlier_score']\n\nfeature_cols = [\n    c for c in df.columns\n    if c not in EXCLUDE_COLS and c not in OTHER_TARGETS and c != TARGET\n    and not any(p in c for p in OUTLIER_PATTERNS)\n    and df[c].dtype in [np.float64, np.int64, np.float32, np.int32]\n]\n\n# Prepare X and y\nX_train, y_train = df_train[feature_cols], df_train[TARGET]\nX_val, y_val = df_val[feature_cols], df_val[TARGET]\nX_test, y_test = df_test[feature_cols], df_test[TARGET]\n\n# Drop all-NaN columns before imputation (SimpleImputer silently drops them,\n# causing shape mismatch when rebuilding the DataFrame)\nall_nan_cols = [c for c in feature_cols if X_train[c].isna().all()]\nif all_nan_cols:\n    print(f'Dropping {len(all_nan_cols)} all-NaN columns: {all_nan_cols}')\n    feature_cols = [c for c in feature_cols if c not in all_nan_cols]\n    X_train = df_train[feature_cols]\n    X_val = df_val[feature_cols]\n    X_test = df_test[feature_cols]\n\n# Imputation and scaling\nimputer = SimpleImputer(strategy='median')\nscaler = StandardScaler()\n\nX_train_imp = pd.DataFrame(imputer.fit_transform(X_train), columns=feature_cols, index=X_train.index)\nX_val_imp = pd.DataFrame(imputer.transform(X_val), columns=feature_cols, index=X_val.index)\nX_test_imp = pd.DataFrame(imputer.transform(X_test), columns=feature_cols, index=X_test.index)\n\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_imp), columns=feature_cols, index=X_train.index)\nX_val_scaled = pd.DataFrame(scaler.transform(X_val_imp), columns=feature_cols, index=X_val.index)\nX_test_scaled = pd.DataFrame(scaler.transform(X_test_imp), columns=feature_cols, index=X_test.index)\n\n# Load and predict with each model\npredictions = {}\n\n# Ridge (uses scaled data)\nridge_path = models_dir / 'ridge_model.pkl'\nif ridge_path.exists():\n    with open(ridge_path, 'rb') as f:\n        ridge_model = pickle.load(f)\n    predictions['Ridge'] = {\n        'val': np.clip(ridge_model.predict(X_val_scaled), 0, None),\n        'test': np.clip(ridge_model.predict(X_test_scaled), 0, None),\n    }\n    print('Ridge loaded')\n\n# LightGBM (uses imputed data, NOT scaled)\nlgb_path = models_dir / 'lightgbm_model.pkl'\nif lgb_path.exists():\n    with open(lgb_path, 'rb') as f:\n        lgb_model = pickle.load(f)\n    predictions['LightGBM'] = {\n        'val': np.clip(lgb_model.predict(X_val_imp), 0, None),\n        'test': np.clip(lgb_model.predict(X_test_imp), 0, None),\n    }\n    print('LightGBM loaded')\n\nprint(f'\\nModels loaded: {list(predictions.keys())}')\nprint(f'Features: {len(feature_cols)}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 2. Final comparison table"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:21:20.051051Z",
     "iopub.status.busy": "2026-02-16T16:21:20.050614Z",
     "iopub.status.idle": "2026-02-16T16:21:20.067162Z",
     "shell.execute_reply": "2026-02-16T16:21:20.065236Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 2.1 — Detailed metrics per model\n# ============================================================\ncomparison = []\n\nfor model_name, preds in predictions.items():\n    row = {'Model': model_name}\n    for split, y_true_arr, y_pred_arr in [\n        ('Val', y_val.values, preds['val']),\n        ('Test', y_test.values, preds['test']),\n    ]:\n        row[f'{split} RMSE'] = np.sqrt(mean_squared_error(y_true_arr, y_pred_arr))\n        row[f'{split} MAE'] = mean_absolute_error(y_true_arr, y_pred_arr)\n        row[f'{split} R2'] = r2_score(y_true_arr, y_pred_arr)\n        # MAPE\n        mask = y_true_arr > 0\n        if mask.any():\n            row[f'{split} MAPE (%)'] = np.mean(\n                np.abs((y_true_arr[mask] - y_pred_arr[mask]) / y_true_arr[mask])\n            ) * 100\n    comparison.append(row)\n\ndf_comp = pd.DataFrame(comparison).round(4)\n\nprint('FINAL MODEL COMPARISON')\nprint('=' * 80)\nprint(df_comp.to_string(index=False))\nprint('=' * 80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:21:20.071756Z",
     "iopub.status.busy": "2026-02-16T16:21:20.071192Z",
     "iopub.status.idle": "2026-02-16T16:21:20.436858Z",
     "shell.execute_reply": "2026-02-16T16:21:20.435129Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 2.2 — Comparative metric chart\n# ============================================================\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\nfig.suptitle('Model Comparison — Key Metrics', fontsize=14)\n\nfor ax, metric_pair, title in zip(axes,\n    [('Val RMSE', 'Test RMSE'), ('Val MAE', 'Test MAE'), ('Val R2', 'Test R2')],\n    ['RMSE (lower = better)', 'MAE (lower = better)', 'R2 (higher = better)']):\n    \n    x = np.arange(len(df_comp))\n    width = 0.35\n    \n    bars1 = ax.bar(x - width/2, df_comp[metric_pair[0]], width, \n                    label='Validation', color='steelblue')\n    bars2 = ax.bar(x + width/2, df_comp[metric_pair[1]], width,\n                    label='Test', color='darkorange')\n    \n    ax.set_title(title)\n    ax.set_xticks(x)\n    ax.set_xticklabels(df_comp['Model'])\n    ax.legend()\n    ax.grid(True, alpha=0.3, axis='y')\n    \n    # Add value annotations\n    for bar in list(bars1) + list(bars2):\n        h = bar.get_height()\n        if not np.isnan(h):\n            ax.annotate(f'{h:.2f}', xy=(bar.get_x() + bar.get_width()/2, h),\n                       ha='center', va='bottom', fontsize=8)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 3. Prediction visualizations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:21:20.440950Z",
     "iopub.status.busy": "2026-02-16T16:21:20.440632Z",
     "iopub.status.idle": "2026-02-16T16:21:20.652651Z",
     "shell.execute_reply": "2026-02-16T16:21:20.650159Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 3.1 — Overlaid predictions on the test set\n# ============================================================\nfig, ax = plt.subplots(figsize=(16, 7))\n\n# Actual values\nax.plot(range(len(y_test)), y_test.values, 'ko-', markersize=4, \n        label='Actual', linewidth=2, zorder=5)\n\n# Predictions from each model\ncolors = {'Ridge': 'blue', 'LightGBM': 'green', 'Prophet': 'purple'}\nmarkers = {'Ridge': 's', 'LightGBM': '^', 'Prophet': 'D'}\n\nfor model_name, preds in predictions.items():\n    color = colors.get(model_name, 'gray')\n    marker = markers.get(model_name, 'o')\n    rmse = np.sqrt(mean_squared_error(y_test.values, preds['test']))\n    ax.plot(range(len(y_test)), preds['test'], f'{color[0]}--{marker}', \n            markersize=3, label=f'{model_name} (RMSE={rmse:.2f})', \n            linewidth=1.5, alpha=0.8)\n\nax.set_title(f'Predictions vs Actual — Test Set ({TARGET})', fontsize=14)\nax.set_xlabel('Temporal index (month x department)')\nax.set_ylabel(TARGET)\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:21:20.656987Z",
     "iopub.status.busy": "2026-02-16T16:21:20.656674Z",
     "iopub.status.idle": "2026-02-16T16:21:20.916371Z",
     "shell.execute_reply": "2026-02-16T16:21:20.914583Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 3.2 — Scatter plot: Predicted vs Actual (best model)\n# ============================================================\n# Identify the best model\nbest_model_name = df_comp.sort_values('Val RMSE').iloc[0]['Model']\nbest_preds = predictions[best_model_name]\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\nfig.suptitle(f'{best_model_name} — Predicted vs Actual', fontsize=14)\n\nfor ax, name, y_true, y_pred in [\n    (axes[0], 'Validation', y_val.values, best_preds['val']),\n    (axes[1], 'Test', y_test.values, best_preds['test']),\n]:\n    ax.scatter(y_true, y_pred, alpha=0.6, s=30, color='steelblue')\n    \n    # y = x line (perfect prediction)\n    lims = [min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())]\n    ax.plot(lims, lims, 'r--', linewidth=2, label='Perfect prediction')\n    \n    r2 = r2_score(y_true, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    ax.set_title(f'{name} (R2={r2:.3f}, RMSE={rmse:.2f})')\n    ax.set_xlabel('Actual')\n    ax.set_ylabel('Predicted')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 4. Feature Importance"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:21:20.921012Z",
     "iopub.status.busy": "2026-02-16T16:21:20.920722Z",
     "iopub.status.idle": "2026-02-16T16:21:21.434329Z",
     "shell.execute_reply": "2026-02-16T16:21:21.432436Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 4.1 — Feature importance comparison\n# ============================================================\nfig, axes = plt.subplots(1, 2, figsize=(16, 8))\nfig.suptitle('Top 15 Features by Model', fontsize=14)\n\n# Ridge: absolute coefficients\nif 'Ridge' in predictions and hasattr(ridge_model, 'coef_'):\n    imp_ridge = pd.Series(np.abs(ridge_model.coef_), index=feature_cols).sort_values(ascending=False)\n    imp_ridge.head(15).iloc[::-1].plot(kind='barh', ax=axes[0], color='steelblue')\n    axes[0].set_title('Ridge — |Coefficients|')\n\n# LightGBM: gain\nif 'LightGBM' in predictions and hasattr(lgb_model, 'feature_importances_'):\n    imp_lgb = pd.Series(lgb_model.feature_importances_, index=feature_cols).sort_values(ascending=False)\n    imp_lgb.head(15).iloc[::-1].plot(kind='barh', ax=axes[1], color='darkgreen')\n    axes[1].set_title('LightGBM — Gain')\n\nfor ax in axes:\n    ax.grid(True, alpha=0.3, axis='x')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:21:21.438217Z",
     "iopub.status.busy": "2026-02-16T16:21:21.437724Z",
     "iopub.status.idle": "2026-02-16T16:21:22.817749Z",
     "shell.execute_reply": "2026-02-16T16:21:22.815869Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 4.2 — SHAP analysis (LightGBM)\n# ============================================================\n# SHAP shows the impact of each feature on EACH individual prediction\n# More informative than global gain importance\n\ntry:\n    import shap\n    \n    explainer = shap.TreeExplainer(lgb_model)\n    shap_values = explainer.shap_values(X_val_imp)\n    \n    # Summary plot: each point = one prediction\n    # Horizontal position = impact on prediction\n    # Color = feature value (red = high, blue = low)\n    fig, ax = plt.subplots(figsize=(12, 8))\n    shap.summary_plot(shap_values, X_val_imp, max_display=20, show=False)\n    plt.title('SHAP — Feature Impact (LightGBM, validation set)', fontsize=12)\n    plt.tight_layout()\n    plt.show()\n    \nexcept ImportError:\n    print('SHAP not available. Install with: pip install shap')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 5. Error analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:21:22.822693Z",
     "iopub.status.busy": "2026-02-16T16:21:22.822112Z",
     "iopub.status.idle": "2026-02-16T16:21:23.377691Z",
     "shell.execute_reply": "2026-02-16T16:21:23.376212Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 5.1 — Residual analysis by model\n# ============================================================\nmodel_names = list(predictions.keys())\nn_models = len(model_names)\n\nfig, axes = plt.subplots(n_models, 2, figsize=(14, 4 * n_models))\nif n_models == 1:\n    axes = axes.reshape(1, -1)\nfig.suptitle('Residual Analysis (test set)', fontsize=14)\n\nfor i, model_name in enumerate(model_names):\n    residuals = y_test.values - predictions[model_name]['test']\n    \n    # Distribution\n    axes[i, 0].hist(residuals, bins=25, edgecolor='black', alpha=0.7)\n    axes[i, 0].axvline(0, color='red', linestyle='--', linewidth=2)\n    axes[i, 0].set_title(f'{model_name} — Residual Distribution')\n    axes[i, 0].set_xlabel('Residual (actual - predicted)')\n    \n    # Residuals vs predictions\n    axes[i, 1].scatter(predictions[model_name]['test'], residuals, alpha=0.5, s=25)\n    axes[i, 1].axhline(0, color='red', linestyle='--', linewidth=2)\n    axes[i, 1].set_title(f'{model_name} — Residuals vs Predictions')\n    axes[i, 1].set_xlabel('Prediction')\n    axes[i, 1].set_ylabel('Residual')\n    axes[i, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:21:23.383343Z",
     "iopub.status.busy": "2026-02-16T16:21:23.382813Z",
     "iopub.status.idle": "2026-02-16T16:21:23.411436Z",
     "shell.execute_reply": "2026-02-16T16:21:23.405585Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 5.2 — Error by department (best model)\n# ============================================================\nbest_preds_test = predictions[best_model_name]['test']\ndf_test_with_preds = df_test.copy()\ndf_test_with_preds['prediction'] = best_preds_test\ndf_test_with_preds['residual'] = df_test_with_preds[TARGET] - df_test_with_preds['prediction']\ndf_test_with_preds['abs_error'] = df_test_with_preds['residual'].abs()\n\ncol_name = 'dept_name' if 'dept_name' in df_test.columns else 'dept'\nerror_by_dept = df_test_with_preds.groupby(col_name).agg({\n    'abs_error': 'mean',\n    'residual': ['mean', 'std'],\n    TARGET: 'mean',\n}).round(2)\nerror_by_dept.columns = ['MAE', 'Mean bias', 'Residual std', f'{TARGET} mean']\nerror_by_dept = error_by_dept.sort_values('MAE', ascending=False)\n\nprint(f'Error by department ({best_model_name}) — Top 20:')\nerror_by_dept.head(20)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:21:23.416295Z",
     "iopub.status.busy": "2026-02-16T16:21:23.415988Z",
     "iopub.status.idle": "2026-02-16T16:21:23.594336Z",
     "shell.execute_reply": "2026-02-16T16:21:23.593256Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 5.3 — Error by month (seasonal error patterns)\n# ============================================================\nerror_by_month = df_test_with_preds.groupby('month').agg({\n    'abs_error': 'mean',\n    'residual': 'mean',\n}).round(2)\n\nmonth_labels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n                'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n\nfig, ax = plt.subplots(figsize=(12, 5))\nbars = ax.bar(range(len(error_by_month)), error_by_month['abs_error'], \n              color='steelblue', edgecolor='black')\nax.set_xticks(range(len(error_by_month)))\nax.set_xticklabels([month_labels[m-1] for m in error_by_month.index])\nax.set_title(f'{best_model_name} — Mean Error by Month (test)', fontsize=14)\nax.set_ylabel('MAE')\nax.grid(True, alpha=0.3, axis='y')\n\n# Add value annotations\nfor bar in bars:\n    h = bar.get_height()\n    ax.annotate(f'{h:.1f}', xy=(bar.get_x() + bar.get_width()/2, h),\n               ha='center', va='bottom', fontsize=9)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 6. Final recommendations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:21:23.598913Z",
     "iopub.status.busy": "2026-02-16T16:21:23.598529Z",
     "iopub.status.idle": "2026-02-16T16:21:23.607561Z",
     "shell.execute_reply": "2026-02-16T16:21:23.606089Z"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# 6.1 — Summary\n# ============================================================\nprint('=' * 70)\nprint('FINAL SUMMARY — Phase 4 ML Modeling')\nprint('=' * 70)\nprint(f'\\nTarget variable: {TARGET}')\nprint(f'Dataset: {len(df)} rows x {len(df.columns)} columns')\nprint(f'Departments: {df[\"dept\"].nunique()}')\nprint(f'Split: Train {len(df_train)} | Val {len(df_val)} | Test {len(df_test)}')\nprint(f'\\nResults:')\nprint('-' * 70)\nprint(f'{\"Model\":15s} | {\"Val RMSE\":>10s} | {\"Test RMSE\":>10s} | {\"Val R2\":>10s} | {\"Rank\":>10s}')\nprint('-' * 70)\n\nsorted_comp = df_comp.sort_values('Val RMSE')\nfor rank, (_, row) in enumerate(sorted_comp.iterrows(), 1):\n    medal = {1: '1st', 2: '2nd', 3: '3rd', 4: '4th'}.get(rank, f'{rank}th')\n    print(f'{row[\"Model\"]:15s} | {row[\"Val RMSE\"]:10.2f} | {row[\"Test RMSE\"]:10.2f} | '\n          f'{row[\"Val R2\"]:10.4f} | {medal:>10s}')\n\nprint('-' * 70)\nprint(f'\\nBest model: {best_model_name}')\nprint('=' * 70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Conclusions and recommendations\n\n### Model hierarchy (96 departments, ~5376 rows):\n\n| Rank | Model | Val R² | Test R² | Strengths | Weaknesses |\n|------|-------|--------|---------|-----------|------------|\n| 1 | **LightGBM** | 0.990 | 0.987 | Captures non-linearities, best overall | Requires careful regularization |\n| 2 | **Ridge** | 0.976 | 0.961 | Robust, interpretable, fast | Linear only |\n| 3 | **Prophet** | 0.616 | 0.530 | Native seasonality decomposition | Per-department training, limited data per series |\n| 4 | **LSTM** | 0.091 | -0.699 | Flexible architecture | Insufficient data, poor generalization |\n\n### Most important features (consensus across 3 methods):\n1. **Temporal lags** (lag_1m of target) — dominant predictor, strong auto-correlation (~0.85)\n2. **Rolling means** (3/6/12 months) — noise smoothing, captures momentum\n3. **Temperature / HDD / CDD** — direct physical impact on HVAC demand\n4. **Household confidence** — proxy for investment intention in home equipment\n5. **Total DPE volume** — real estate activity indicator (volume scaler)\n6. **Economic indicators** (IPI C28, business climate) — industry-level context\n\n### Key findings:\n- **No overfitting detected**: LightGBM R² gap = 0.5%, Ridge R² gap = 1.2% — both HEALTHY\n- **CV stability confirmed**: LightGBM CoV = 4.7%, Ridge CoV = 9.5% — both STABLE across temporal folds\n- **GridSearchCV validated**: manual hyperparameters are optimal across 144 combinations\n- **Prophet limitation**: per-department training with only ~36 months per series is insufficient\n- **LSTM limitation**: ablation study (7 configs) proves the issue is data volume, not tuning\n\n### Error analysis insights:\n- Largest errors on high-volume departments (Ile-de-France) — absolute error scales with volume\n- Seasonal error pattern: slightly higher errors in autumn (Oct-Nov) when heat pump demand peaks\n- Residuals are approximately normally distributed (no systematic bias)\n\n### Improvement suggestions:\n- **Ensemble** Ridge + LightGBM (weighted average) — may capture complementary patterns\n- **Prediction confidence intervals** via quantile regression or conformal prediction\n- **Error-weighted retraining**: give higher weight to high-volume departments where absolute error matters most\n- **Regional grouping**: analyze prediction quality by region (AURA, IDF, etc.) for deployment priorities\n- **MAPE-based evaluation** alongside RMSE for percentage-error perspective\n- **Monthly model retraining** with new DPE data to capture market evolution\n\n### Production recommendations:\n- **Recommended model**: LightGBM (best performance, good balance of accuracy and robustness)\n- **Interpretability fallback**: Ridge (coefficients directly show feature impact)\n- **Retraining frequency**: monthly with new DPE data\n- **Next step**: Full feature review with SHAP analysis (Notebook 05)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 — Feature Review and SHAP Analysis\n",
    "## HVAC Market Analysis — Metropolitan France (96 departments)\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This notebook fulfills the **CLAUDE.md requirement** for a collaborative column review.\n",
    "It documents:\n",
    "\n",
    "1. **Every selected column** in the ML dataset, its content, and relevance\n",
    "2. **Keep/drop/add decisions** with justification\n",
    "3. **Feature importance analysis** (Ridge coefficients, LightGBM gain, SHAP values)\n",
    "4. **Evaluation of new candidate variables** (revenu_median, prix_m2, nb_logements_total, pct_maisons, SITADEL)\n",
    "\n",
    "This review is a **portfolio deliverable** — clear, readable, and well-documented."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# IMPORTS\n",
    "# ============================================================\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print('Imports OK')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load dataset and models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 1.1 — Load ML dataset and features dataset\n",
    "# ============================================================\n",
    "df_ml = pd.read_csv('../data/features/hvac_ml_dataset.csv')\n",
    "df_feat = pd.read_csv('../data/features/hvac_features_dataset.csv')\n",
    "\n",
    "print(f'ML dataset: {df_ml.shape[0]} rows x {df_ml.shape[1]} columns')\n",
    "print(f'Features dataset: {df_feat.shape[0]} rows x {df_feat.shape[1]} columns')\n",
    "print(f'Departments: {df_ml[\"dept\"].nunique()}')\n",
    "print(f'\\nML dataset columns: {sorted(df_ml.columns.tolist())}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 1.2 — Load trained models\n",
    "# ============================================================\n",
    "models_dir = Path('../data/models')\n",
    "\n",
    "ridge_model = None\n",
    "lgb_model = None\n",
    "\n",
    "ridge_path = models_dir / 'ridge_model.pkl'\n",
    "if ridge_path.exists():\n",
    "    with open(ridge_path, 'rb') as f:\n",
    "        ridge_model = pickle.load(f)\n",
    "    print(f'Ridge model loaded ({len(ridge_model.coef_)} features)')\n",
    "\n",
    "lgb_path = models_dir / 'lightgbm_model.pkl'\n",
    "if lgb_path.exists():\n",
    "    with open(lgb_path, 'rb') as f:\n",
    "        lgb_model = pickle.load(f)\n",
    "    print(f'LightGBM model loaded ({lgb_model.n_features_} features)')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Column inventory and classification\n",
    "\n",
    "Every column in the ML dataset is categorized and documented."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 2.1 — Classify all columns by category\n",
    "# ============================================================\n",
    "COLUMN_CATEGORIES = {\n",
    "    'Identifiers': {\n",
    "        'date_id': 'Temporal key (YYYYMM format) — used for splitting, not as feature',\n",
    "        'dept': 'Department code — used for grouping, not as feature',\n",
    "        'dept_name': 'Department name — metadata, not a feature',\n",
    "        'city_ref': 'Reference city (prefecture) — metadata',\n",
    "        'latitude': 'City latitude — geographic metadata',\n",
    "        'longitude': 'City longitude — geographic metadata',\n",
    "    },\n",
    "    'Target variables': {\n",
    "        'nb_dpe_total': 'Total DPE count per month/dept — proxy for real estate activity',\n",
    "        'nb_installations_pac': 'DPE with heat pump — PRIMARY TARGET',\n",
    "        'nb_installations_clim': 'DPE with air conditioning',\n",
    "        'nb_dpe_classe_ab': 'DPE class A or B (high performance)',\n",
    "        'pct_pac': 'Heat pump rate (%) — derived from targets',\n",
    "        'pct_clim': 'AC rate (%) — derived from targets',\n",
    "        'pct_classe_ab': 'Class A-B rate (%) — derived from targets',\n",
    "    },\n",
    "    'Weather (local, per dept)': {\n",
    "        'temp_mean': 'Monthly mean temperature (C) — direct impact on heating/cooling demand',\n",
    "        'temp_max': 'Monthly max temperature (C) — heatwave indicator',\n",
    "        'temp_min': 'Monthly min temperature (C) — frost indicator',\n",
    "        'hdd_sum': 'Heating Degree Days (base 18C) — heating demand proxy',\n",
    "        'cdd_sum': 'Cooling Degree Days (base 18C) — cooling demand proxy',\n",
    "        'precipitation_sum': 'Monthly precipitation sum (mm)',\n",
    "        'nb_jours_canicule': 'Heatwave days (>35C) — AC demand driver',\n",
    "        'nb_jours_gel': 'Frost days (<0C) — heating demand driver',\n",
    "    },\n",
    "    'Economic (national)': {\n",
    "        'confiance_menages': 'Household confidence index (INSEE) — investment intention proxy',\n",
    "        'climat_affaires_indus': 'Business climate, industry (INSEE)',\n",
    "        'climat_affaires_bat': 'Business climate, construction (INSEE) — sector-specific',\n",
    "        'ipi_manufacturing': 'Industrial Production Index, manufacturing (INSEE)',\n",
    "        'ipi_hvac_c28': 'IPI HVAC sector C28 (Eurostat) — industry activity',\n",
    "        'ipi_hvac_c2825': 'IPI HVAC sub-sector C2825 (Eurostat) — specific HVAC production',\n",
    "    },\n",
    "    'Calendar': {\n",
    "        'year': 'Year — captures long-term trend',\n",
    "        'month': 'Month number (1-12)',\n",
    "        'quarter': 'Quarter (1-4)',\n",
    "        'is_heating': 'Binary: heating season (Oct-Mar)',\n",
    "        'is_cooling': 'Binary: cooling season (Jun-Sep)',\n",
    "        'month_sin': 'Cyclical encoding of month (sin component)',\n",
    "        'month_cos': 'Cyclical encoding of month (cos component)',\n",
    "    },\n",
    "    'SITADEL (construction, per dept)': {\n",
    "        'nb_logements_autorises': 'Total authorized housing units — construction activity',\n",
    "        'nb_logements_individuels': 'Authorized individual housing — houses (more heat pumps)',\n",
    "        'nb_logements_collectifs': 'Authorized collective housing — apartments',\n",
    "        'surface_autorisee_m2': 'Total authorized surface (m2) — construction volume',\n",
    "    },\n",
    "    'INSEE Filosofi reference (static, per dept)': {\n",
    "        'revenu_median': 'Median income — linked to aid eligibility (MaPrimeRenov, CEE)',\n",
    "        'prix_m2_median': 'Median price/m2 — real estate market activity proxy',\n",
    "        'nb_logements_total': 'Total housing stock — normalization base',\n",
    "        'pct_maisons': 'Percentage of houses (vs apartments) — structural driver',\n",
    "    },\n",
    "}\n",
    "\n",
    "# Display the inventory\n",
    "for category, cols in COLUMN_CATEGORIES.items():\n",
    "    present = [c for c in cols if c in df_ml.columns]\n",
    "    missing = [c for c in cols if c not in df_ml.columns]\n",
    "    print(f'\\n=== {category} ({len(present)}/{len(cols)} present) ===')\n",
    "    for col, desc in cols.items():\n",
    "        status = 'OK' if col in df_ml.columns else 'MISSING'\n",
    "        print(f'  [{status:7s}] {col:35s} : {desc}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Feature importance analysis\n",
    "\n",
    "We compare three importance methods:\n",
    "1. **Ridge |coefficients|** — linear importance after standardization\n",
    "2. **LightGBM gain** — total split gain per feature\n",
    "3. **SHAP values** — game-theoretic attribution per prediction"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ============================================================\n# 3.1 — Prepare features for importance analysis\n# ============================================================\nTARGET = 'nb_installations_pac'\nTRAIN_END = 202406\n\nEXCLUDE_COLS = {\n    'date_id', 'dept', 'dept_name', 'city_ref', 'latitude', 'longitude',\n    'n_valid_features', 'pct_valid_features',\n    'nb_installations_clim', 'nb_dpe_total', 'nb_dpe_classe_ab',\n    'pct_pac', 'pct_clim', 'pct_classe_ab',\n}\nOUTLIER_PATTERNS = ['_outlier_iqr', '_outlier_zscore', '_outlier_iforest',\n                    '_outlier_consensus', '_outlier_score']\n\nfeature_cols = [\n    c for c in df_feat.columns\n    if c not in EXCLUDE_COLS and c != TARGET\n    and not any(p in c for p in OUTLIER_PATTERNS)\n    and df_feat[c].dtype in [np.float64, np.int64, np.float32, np.int32]\n]\n\ndf_train = df_feat[df_feat['date_id'] <= TRAIN_END]\nX_train = df_train[feature_cols]\n\n# Drop all-NaN columns before imputation (SimpleImputer silently drops them,\n# causing shape mismatch when rebuilding the DataFrame)\nall_nan_cols = [c for c in feature_cols if X_train[c].isna().all()]\nif all_nan_cols:\n    print(f'Dropping {len(all_nan_cols)} all-NaN columns: {all_nan_cols}')\n    feature_cols = [c for c in feature_cols if c not in all_nan_cols]\n    X_train = df_train[feature_cols]\n\n# Imputation for SHAP\nimputer = SimpleImputer(strategy='median')\nX_train_imp = pd.DataFrame(\n    imputer.fit_transform(X_train), columns=feature_cols, index=X_train.index\n)\n\nprint(f'Feature columns for importance: {len(feature_cols)}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 3.2 — Ridge importance (absolute coefficients)\n",
    "# ============================================================\n",
    "if ridge_model is not None and len(ridge_model.coef_) == len(feature_cols):\n",
    "    imp_ridge = pd.Series(\n",
    "        np.abs(ridge_model.coef_), index=feature_cols\n",
    "    ).sort_values(ascending=False)\n",
    "    \n",
    "    print('Ridge — Top 25 features by |coefficient|:')\n",
    "    print('-' * 60)\n",
    "    for i, (feat, val) in enumerate(imp_ridge.head(25).items(), 1):\n",
    "        sign = '+' if ridge_model.coef_[feature_cols.index(feat)] > 0 else '-'\n",
    "        print(f'  {i:2d}. {feat:45s} {sign}{val:.4f}')\n",
    "else:\n",
    "    print('Ridge model not available or feature mismatch.')\n",
    "    print(f'  Model features: {len(ridge_model.coef_) if ridge_model else \"N/A\"}')\n",
    "    print(f'  Current features: {len(feature_cols)}')\n",
    "    imp_ridge = None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 3.3 — LightGBM importance (gain)\n",
    "# ============================================================\n",
    "if lgb_model is not None and lgb_model.n_features_ == len(feature_cols):\n",
    "    imp_lgb = pd.Series(\n",
    "        lgb_model.feature_importances_, index=feature_cols\n",
    "    ).sort_values(ascending=False)\n",
    "    \n",
    "    print('LightGBM — Top 25 features by gain:')\n",
    "    print('-' * 60)\n",
    "    for i, (feat, val) in enumerate(imp_lgb.head(25).items(), 1):\n",
    "        print(f'  {i:2d}. {feat:45s} {val:.0f}')\n",
    "else:\n",
    "    print('LightGBM model not available or feature mismatch.')\n",
    "    imp_lgb = None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 3.4 — SHAP values (LightGBM)\n",
    "# ============================================================\n",
    "if lgb_model is not None and lgb_model.n_features_ == len(feature_cols):\n",
    "    import shap\n",
    "    \n",
    "    explainer = shap.TreeExplainer(lgb_model)\n",
    "    # Use a sample for SHAP (faster)\n",
    "    sample_size = min(500, len(X_train_imp))\n",
    "    X_sample = X_train_imp.sample(sample_size, random_state=42)\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "    \n",
    "    # Summary plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    shap.summary_plot(shap_values, X_sample, max_display=25, show=False)\n",
    "    plt.title('SHAP — Feature impact on predictions (LightGBM)', fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Mean absolute SHAP values\n",
    "    shap_importance = pd.Series(\n",
    "        np.abs(shap_values).mean(axis=0), index=feature_cols\n",
    "    ).sort_values(ascending=False)\n",
    "    \n",
    "    print('\\nTop 25 features by mean |SHAP|:')\n",
    "    print('-' * 60)\n",
    "    for i, (feat, val) in enumerate(shap_importance.head(25).items(), 1):\n",
    "        print(f'  {i:2d}. {feat:45s} {val:.4f}')\n",
    "else:\n",
    "    print('SHAP analysis skipped (model not available or feature mismatch).')\n",
    "    shap_importance = None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 3.5 — Combined importance comparison\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 10))\n",
    "fig.suptitle('Feature Importance — 3 Methods Compared (Top 20)', fontsize=14)\n",
    "\n",
    "if imp_ridge is not None:\n",
    "    imp_ridge.head(20).iloc[::-1].plot(kind='barh', ax=axes[0], color='steelblue')\n",
    "    axes[0].set_title('Ridge |coefficients|')\n",
    "\n",
    "if imp_lgb is not None:\n",
    "    imp_lgb.head(20).iloc[::-1].plot(kind='barh', ax=axes[1], color='darkgreen')\n",
    "    axes[1].set_title('LightGBM gain')\n",
    "\n",
    "if shap_importance is not None:\n",
    "    shap_importance.head(20).iloc[::-1].plot(kind='barh', ax=axes[2], color='darkorange')\n",
    "    axes[2].set_title('Mean |SHAP|')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Column-level keep/drop decisions\n",
    "\n",
    "Based on the importance analysis, correlation study (notebook 01), and domain knowledge,\n",
    "here are the keep/drop decisions for each feature category."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 4.1 — Feature decisions table\n",
    "# ============================================================\n",
    "FEATURE_DECISIONS = [\n",
    "    # (feature, decision, category, justification)\n",
    "    # --- DPE features ---\n",
    "    ('nb_dpe_total', 'KEEP as target', 'DPE', 'Primary volume indicator, strong predictor'),\n",
    "    ('nb_installations_pac', 'TARGET', 'DPE', 'Primary prediction target'),\n",
    "    ('nb_installations_clim', 'KEEP as target', 'DPE', 'Secondary target, useful for multi-target'),\n",
    "    ('nb_dpe_classe_ab', 'KEEP as target', 'DPE', 'Quality indicator'),\n",
    "    ('pct_pac', 'DROP from features', 'DPE', 'Derived from target — leakage risk'),\n",
    "    ('pct_clim', 'DROP from features', 'DPE', 'Derived from target — leakage risk'),\n",
    "    ('pct_classe_ab', 'DROP from features', 'DPE', 'Derived from target — leakage risk'),\n",
    "    \n",
    "    # --- Weather ---\n",
    "    ('temp_mean', 'KEEP', 'Weather', 'Direct impact on heating/cooling demand'),\n",
    "    ('temp_max', 'KEEP', 'Weather', 'Heatwave indicator, drives AC demand'),\n",
    "    ('temp_min', 'KEEP', 'Weather', 'Frost indicator, drives heating demand'),\n",
    "    ('hdd_sum', 'KEEP', 'Weather', 'Heating Degree Days — strong predictor per domain logic'),\n",
    "    ('cdd_sum', 'KEEP', 'Weather', 'Cooling Degree Days — AC demand proxy'),\n",
    "    ('precipitation_sum', 'KEEP', 'Weather', 'Weather context, low importance but no cost'),\n",
    "    ('nb_jours_canicule', 'KEEP', 'Weather', 'Extreme heat days — AC demand driver'),\n",
    "    ('nb_jours_gel', 'KEEP', 'Weather', 'Extreme cold days — heating demand driver'),\n",
    "    \n",
    "    # --- Economic ---\n",
    "    ('confiance_menages', 'KEEP', 'Economic', 'Household confidence — investment intention proxy'),\n",
    "    ('climat_affaires_indus', 'KEEP', 'Economic', 'Industry business climate'),\n",
    "    ('climat_affaires_bat', 'KEEP', 'Economic', 'Construction business climate — sector-specific'),\n",
    "    ('ipi_manufacturing', 'KEEP', 'Economic', 'Manufacturing production index'),\n",
    "    ('ipi_hvac_c28', 'KEEP', 'Economic', 'HVAC-specific industrial production'),\n",
    "    ('ipi_hvac_c2825', 'KEEP', 'Economic', 'HVAC sub-sector production'),\n",
    "    \n",
    "    # --- Calendar ---\n",
    "    ('month', 'KEEP', 'Calendar', 'Seasonality encoding'),\n",
    "    ('quarter', 'KEEP', 'Calendar', 'Quarterly patterns'),\n",
    "    ('year', 'KEEP', 'Calendar', 'Long-term trend'),\n",
    "    ('is_heating', 'KEEP', 'Calendar', 'Heating season binary'),\n",
    "    ('is_cooling', 'KEEP', 'Calendar', 'Cooling season binary'),\n",
    "    ('month_sin', 'KEEP', 'Calendar', 'Cyclical month encoding (sin)'),\n",
    "    ('month_cos', 'KEEP', 'Calendar', 'Cyclical month encoding (cos)'),\n",
    "    \n",
    "    # --- SITADEL (NEW) ---\n",
    "    ('nb_logements_autorises', 'KEEP', 'SITADEL', 'Construction activity — EVALUATE with SHAP'),\n",
    "    ('nb_logements_individuels', 'KEEP', 'SITADEL', 'Individual housing — houses get more heat pumps'),\n",
    "    ('nb_logements_collectifs', 'KEEP', 'SITADEL', 'Collective housing — constrained by co-ownership'),\n",
    "    ('surface_autorisee_m2', 'KEEP', 'SITADEL', 'Construction volume — EVALUATE with SHAP'),\n",
    "    \n",
    "    # --- INSEE Filosofi reference (NEW) ---\n",
    "    ('revenu_median', 'KEEP', 'Reference', 'Median income — linked to aid eligibility'),\n",
    "    ('prix_m2_median', 'KEEP', 'Reference', 'Price/m2 — real estate market proxy'),\n",
    "    ('nb_logements_total', 'KEEP', 'Reference', 'Housing stock — normalization base'),\n",
    "    ('pct_maisons', 'KEEP', 'Reference', 'House % — structural driver (houses -> more PAC)'),\n",
    "]\n",
    "\n",
    "df_decisions = pd.DataFrame(FEATURE_DECISIONS, \n",
    "                            columns=['Feature', 'Decision', 'Category', 'Justification'])\n",
    "\n",
    "print(f'Feature decisions: {len(df_decisions)} columns reviewed')\n",
    "print(f'  KEEP: {(df_decisions[\"Decision\"] == \"KEEP\").sum()}')\n",
    "print(f'  DROP: {df_decisions[\"Decision\"].str.contains(\"DROP\").sum()}')\n",
    "print(f'  TARGET: {df_decisions[\"Decision\"].str.contains(\"TARGET|target\").sum()}')\n",
    "print()\n",
    "display(df_decisions)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Evaluation of new candidate variables\n",
    "\n",
    "These variables were added per CLAUDE.md recommendations. We evaluate their contribution."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 5.1 — New features: presence and statistics\n",
    "# ============================================================\n",
    "NEW_FEATURES = {\n",
    "    'revenu_median': 'INSEE Filosofi — directly linked to MaPrimeRenov aid eligibility',\n",
    "    'prix_m2_median': 'INSEE — proxy for real estate market activity and housing type',\n",
    "    'nb_logements_total': 'INSEE Recensement — total housing stock for normalization',\n",
    "    'pct_maisons': 'INSEE — houses vs apartments, structural HVAC driver',\n",
    "    'nb_logements_autorises': 'SITADEL — construction permits, new housing activity',\n",
    "    'nb_logements_individuels': 'SITADEL — individual housing (more heat pump friendly)',\n",
    "    'nb_logements_collectifs': 'SITADEL — collective housing (constrained installations)',\n",
    "    'surface_autorisee_m2': 'SITADEL — total authorized surface for construction',\n",
    "}\n",
    "\n",
    "print('New candidate variables evaluation:')\n",
    "print('=' * 80)\n",
    "for feat, desc in NEW_FEATURES.items():\n",
    "    if feat in df_ml.columns:\n",
    "        series = df_ml[feat]\n",
    "        corr = series.corr(df_ml[TARGET])\n",
    "        null_pct = series.isna().mean() * 100\n",
    "        print(f'\\n  {feat}')\n",
    "        print(f'    Description: {desc}')\n",
    "        print(f'    Present: YES | NaN: {null_pct:.1f}% | Corr with target: {corr:+.3f}')\n",
    "        print(f'    Stats: mean={series.mean():.1f}, std={series.std():.1f}, '\n",
    "              f'min={series.min():.1f}, max={series.max():.1f}')\n",
    "    else:\n",
    "        print(f'\\n  {feat}')\n",
    "        print(f'    Description: {desc}')\n",
    "        print(f'    Present: NO — run \"python -m src.pipeline process\" to generate')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 5.2 — Correlation of new features with target\n",
    "# ============================================================\n",
    "available_new = [f for f in NEW_FEATURES if f in df_ml.columns and df_ml[f].notna().sum() > 10]\n",
    "\n",
    "if available_new:\n",
    "    n = len(available_new)\n",
    "    ncols = min(4, n)\n",
    "    nrows = (n + ncols - 1) // ncols\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(4 * ncols, 4 * nrows))\n",
    "    if nrows == 1 and ncols == 1:\n",
    "        axes = np.array([axes])\n",
    "    axes = axes.flat\n",
    "    fig.suptitle(f'New features vs {TARGET}', fontsize=14)\n",
    "    \n",
    "    for ax, feat in zip(axes, available_new):\n",
    "        mask = df_ml[[feat, TARGET]].dropna().index\n",
    "        ax.scatter(df_ml.loc[mask, feat], df_ml.loc[mask, TARGET], alpha=0.2, s=8)\n",
    "        r = df_ml[feat].corr(df_ml[TARGET])\n",
    "        ax.set_title(f'{feat}\\n(r={r:+.3f})', fontsize=9)\n",
    "        ax.set_xlabel(feat, fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty axes\n",
    "    for i in range(n, nrows * ncols):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No new features available in dataset. Run the pipeline first.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 6. Conclusions and recommendations\n\n### Feature review summary\n\n| Category | Count | Decision | Rationale |\n|----------|-------|----------|-----------|\n| Identifiers | 6 | EXCLUDE from features | Metadata only (dept, date_id, lat/lon) |\n| Targets | 7 | TARGET / EXCLUDE | Prediction targets or derived ratios (leakage risk) |\n| Weather | 8 | **KEEP all** | Direct physical drivers of HVAC demand (HDD, CDD, temp) |\n| Economic | 6 | **KEEP all** | National investment context (confidence, IPI, business climate) |\n| Calendar | 7 | **KEEP all** | Seasonality and trend capture (month, quarter, year, sin/cos) |\n| SITADEL | 4 | **KEEP, EVALUATE** | Construction activity — evaluate impact with SHAP |\n| Reference | 4 | **KEEP, EVALUATE** | Socioeconomic context (altitude, density) — evaluate with SHAP |\n\n### Key findings from importance analysis\n\n1. **Temporal lags dominate** across all 3 methods (Ridge, LightGBM, SHAP) — lag_1m is the single most predictive feature, reflecting strong month-to-month auto-correlation in heat pump installations\n2. **Weather features consistently rank high** — HDD (heating degree days), temperature, and CDD capture the physical relationship between climate and HVAC demand\n3. **Economic indicators provide moderate but consistent signal** — household confidence and IPI HVAC (C28) appear in the top 15 across methods\n4. **Calendar features are important** — month (sin/cos encoding) captures seasonality, year captures the long-term growth trend\n5. **Leakage risk confirmed**: pct_pac, pct_clim, pct_classe_ab are derived from the target and must be excluded from features\n\n### SITADEL & Reference features status\n- **Column naming fix applied**: the 3 column mismatches (DATE_REELLE_AUTORISATION, NB_LGT_COL_CREES, SURF_HAB_CREEE) have been resolved in the collector\n- **To integrate**: run `python -m src.pipeline merge` then `python -m src.pipeline features` to regenerate the ML dataset with SITADEL and reference features\n- **Expected impact**: SITADEL features (construction permits) should add department-level temporal signal; reference features (altitude, density) should help explain inter-department variance\n\n### Improvement suggestions:\n- **Recursive Feature Elimination (RFE)**: systematically test removing the least important features — may simplify the model without losing performance\n- **Boruta feature selection**: all-relevant feature selection to distinguish truly informative features from noise\n- **Partial Dependence Plots (PDP)**: visualize non-linear relationships for top 5 LightGBM features\n- **Feature interaction analysis**: test explicit interaction terms (e.g., HDD x department_type, confidence x season)\n- **SHAP interaction values**: identify the most important feature pairs (shap.TreeExplainer.shap_interaction_values)\n\n### Next steps (with full data)\n\nWhen SITADEL/reference features are integrated:\n1. Re-run SHAP analysis to evaluate new features' contribution\n2. Apply feature selection (keep only features with significant SHAP importance)\n3. Re-evaluate model performance with the enriched feature set\n4. Consider feature stability analysis (do importances change across temporal CV folds?)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
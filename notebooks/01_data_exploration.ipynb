{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 â€” Exploratory Data Analysis (EDA)\n",
    "## HVAC Market Analysis â€” Metropolitan France (96 departments)\n",
    "\n",
    "**Objective**: Explore and understand the HVAC ML dataset before modeling.\n",
    "\n",
    "**Plan**:\n",
    "1. Load and overview the dataset\n",
    "2. Missing values analysis\n",
    "3. Distribution of target variables\n",
    "4. Time series and trends\n",
    "5. Seasonality of the HVAC market\n",
    "6. Department comparison\n",
    "7. Correlations between variables\n",
    "8. New features: SITADEL (construction) and INSEE Filosofi (socioeconomic)\n",
    "9. Conclusions for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:18:54.099369Z",
     "iopub.status.busy": "2026-02-16T16:18:54.099078Z",
     "iopub.status.idle": "2026-02-16T16:18:57.074360Z",
     "shell.execute_reply": "2026-02-16T16:18:57.072485Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS AND CONFIGURATION\n",
    "# ============================================================\n",
    "import sys\n",
    "sys.path.insert(0, '..')  # To import project modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "# Full DataFrame display\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print('Imports OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load and overview the dataset\n",
    "\n",
    "The ML-ready dataset contains data aggregated by **month x department**:\n",
    "- 96 metropolitan French departments\n",
    "- DPE v2 period: July 2021 onwards\n",
    "- Target variables: heat pump installations, air conditioning, total DPE count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:18:57.123778Z",
     "iopub.status.busy": "2026-02-16T16:18:57.123243Z",
     "iopub.status.idle": "2026-02-16T16:18:57.139189Z",
     "shell.execute_reply": "2026-02-16T16:18:57.137434Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1.1 â€” Load the ML-ready dataset\n",
    "# ============================================================\n",
    "# This dataset is produced by: python -m src.pipeline merge\n",
    "# Grain: month x department (96 depts x N months)\n",
    "\n",
    "df = pd.read_csv('../data/features/hvac_ml_dataset.csv')\n",
    "\n",
    "print(f'ML-ready dataset: {df.shape[0]} rows x {df.shape[1]} columns')\n",
    "print(f'Period: {df[\"date_id\"].min()} -> {df[\"date_id\"].max()}')\n",
    "print(f'Departments: {len(df[\"dept\"].unique())} unique')\n",
    "print(f'\\nColumn types:')\n",
    "print(df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:18:57.142442Z",
     "iopub.status.busy": "2026-02-16T16:18:57.141994Z",
     "iopub.status.idle": "2026-02-16T16:18:57.162266Z",
     "shell.execute_reply": "2026-02-16T16:18:57.160596Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1.2 â€” Preview the first rows\n",
    "# ============================================================\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:18:57.166377Z",
     "iopub.status.busy": "2026-02-16T16:18:57.166103Z",
     "iopub.status.idle": "2026-02-16T16:18:57.208541Z",
     "shell.execute_reply": "2026-02-16T16:18:57.207172Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1.3 â€” Global descriptive statistics\n",
    "# ============================================================\n",
    "df.describe().T.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.4 â€” Data Quality Audit: NaN Detection & Traceability\n",
    "\n",
    "**Objective**: Systematically detect all NaN in the dataset and trace their origin back to the source data.\n",
    "\n",
    "This audit ensures:\n",
    "- Every NaN is explained (collection gap, feature engineering lag, missing source)\n",
    "- Department coverage is verified (96/96 expected)\n",
    "- Temporal coverage is complete (no missing months)\n",
    "- Any column with >5% NaN is flagged for investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1.4a â€” Per-column NaN Count & Percentage\n",
    "# ============================================================\n",
    "nan_count = df.isna().sum()\n",
    "nan_pct = (df.isna().mean() * 100).round(2)\n",
    "nan_report = pd.DataFrame({\n",
    "    'NaN_count': nan_count,\n",
    "    'NaN_pct': nan_pct,\n",
    "    'dtype': df.dtypes\n",
    "}).sort_values('NaN_pct', ascending=False)\n",
    "\n",
    "# Flag severity levels\n",
    "nan_report['severity'] = nan_report['NaN_pct'].apply(\n",
    "    lambda x: 'ðŸ”´ CRITICAL (>50%)' if x > 50\n",
    "    else 'ðŸŸ¡ WARNING (>5%)' if x > 5\n",
    "    else 'ðŸŸ¢ OK' if x > 0\n",
    "    else 'âœ… CLEAN'\n",
    ")\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  NaN AUDIT â€” ML-ready dataset ({df.shape[0]} rows Ã— {df.shape[1]} cols)\")\n",
    "print(f\"{'='*70}\")\n",
    "total_nan = nan_count.sum()\n",
    "total_cells = df.shape[0] * df.shape[1]\n",
    "print(f\"Total NaN: {total_nan:,} / {total_cells:,} cells ({100*total_nan/total_cells:.2f}%)\")\n",
    "print(f\"Columns with NaN: {(nan_pct > 0).sum()} / {len(nan_pct)}\")\n",
    "print()\n",
    "\n",
    "# Show problematic columns\n",
    "problematic = nan_report[nan_report['NaN_pct'] > 0]\n",
    "if len(problematic) > 0:\n",
    "    print(\"Columns with NaN (sorted by severity):\")\n",
    "    display(problematic[['NaN_count', 'NaN_pct', 'severity']])\n",
    "else:\n",
    "    print(\"âœ… No NaN in the dataset â€” all clean!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1.4b â€” NaN Provenance: Trace each NaN back to its source\n",
    "# ============================================================\n",
    "# Classify columns by data source to identify where NaN originate\n",
    "\n",
    "SOURCE_PATTERNS = {\n",
    "    'Weather (Open-Meteo)': ['temp_', 'hdd', 'cdd', 'precip', 'wind', 'pac_inefficient',\n",
    "                              'elevation', 'extreme_cold', 'extreme_hot', 'frost_days'],\n",
    "    'DPE (ADEME)': ['nb_dpe', 'nb_installations', 'pct_pac', 'pct_clim', 'pct_classe'],\n",
    "    'INSEE (economic)': ['confiance_menages', 'climat_affaires', 'chomage', 'permis_construire',\n",
    "                          'prix_immobilier', 'taux_pauvrete', 'revenu_median', 'densite_pop'],\n",
    "    'Eurostat (industry)': ['ipi_', 'eurostat'],\n",
    "    'SITADEL (construction)': ['sitadel', 'logements_autorises', 'logements_commences'],\n",
    "    'Feature eng. (lag/rolling)': ['lag_', 'rolling_', 'diff_', 'trend_', 'interact_',\n",
    "                                     'ratio_', 'seasonal_', 'momentum_'],\n",
    "}\n",
    "\n",
    "def classify_column(col_name):\n",
    "    \"\"\"Identify the data source of a column by pattern matching.\"\"\"\n",
    "    for source, patterns in SOURCE_PATTERNS.items():\n",
    "        if any(p in col_name.lower() for p in patterns):\n",
    "            return source\n",
    "    return 'Other / Identifier'\n",
    "\n",
    "nan_by_source = {}\n",
    "for col in df.columns:\n",
    "    source = classify_column(col)\n",
    "    if source not in nan_by_source:\n",
    "        nan_by_source[source] = {'cols_total': 0, 'cols_with_nan': 0, 'nan_cells': 0}\n",
    "    nan_by_source[source]['cols_total'] += 1\n",
    "    if df[col].isna().sum() > 0:\n",
    "        nan_by_source[source]['cols_with_nan'] += 1\n",
    "        nan_by_source[source]['nan_cells'] += df[col].isna().sum()\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  NaN PROVENANCE â€” By data source\")\n",
    "print(f\"{'='*70}\")\n",
    "source_df = pd.DataFrame(nan_by_source).T\n",
    "source_df['nan_pct_of_total'] = (source_df['nan_cells'] / total_nan * 100).round(1) if total_nan > 0 else 0\n",
    "display(source_df)\n",
    "\n",
    "# Detailed breakdown: show affected columns per source\n",
    "print(\"\\n--- Detailed breakdown: affected columns per source ---\")\n",
    "for source, patterns in SOURCE_PATTERNS.items():\n",
    "    affected = [\n",
    "        col for col in df.columns\n",
    "        if classify_column(col) == source and df[col].isna().sum() > 0\n",
    "    ]\n",
    "    if affected:\n",
    "        print(f\"\\nðŸ“Œ {source} ({len(affected)} column(s) with NaN):\")\n",
    "        for col in affected:\n",
    "            pct = 100 * df[col].isna().mean()\n",
    "            print(f\"   {col:40s} â†’ {pct:5.1f}% NaN ({df[col].isna().sum()} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1.4c â€” Department Coverage Check (96 metropolitan expected)\n",
    "# ============================================================\n",
    "EXPECTED_DEPTS = {\n",
    "    '01','02','03','04','05','06','07','08','09','10',\n",
    "    '11','12','13','14','15','16','17','18','19',\n",
    "    '21','22','23','24','25','26','27','28','29',\n",
    "    '2A','2B',\n",
    "    '30','31','32','33','34','35','36','37','38','39',\n",
    "    '40','41','42','43','44','45','46','47','48','49',\n",
    "    '50','51','52','53','54','55','56','57','58','59',\n",
    "    '60','61','62','63','64','65','66','67','68','69',\n",
    "    '70','71','72','73','74','75','76','77','78','79',\n",
    "    '80','81','82','83','84','85','86','87','88','89',\n",
    "    '90','91','92','93','94','95',\n",
    "}\n",
    "\n",
    "actual_depts = set(df['dept'].astype(str).unique())\n",
    "missing_depts = EXPECTED_DEPTS - actual_depts\n",
    "extra_depts = actual_depts - EXPECTED_DEPTS\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  DEPARTMENT COVERAGE CHECK\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Expected: {len(EXPECTED_DEPTS)} departments\")\n",
    "print(f\"Found:    {len(actual_depts)} departments\")\n",
    "\n",
    "if not missing_depts:\n",
    "    print(\"âœ… All 96 metropolitan departments are present!\")\n",
    "else:\n",
    "    print(f\"ðŸ”´ MISSING {len(missing_depts)} departments: {sorted(missing_depts)}\")\n",
    "\n",
    "if extra_depts:\n",
    "    print(f\"âš ï¸  Extra departments (unexpected): {sorted(extra_depts)}\")\n",
    "\n",
    "# Check rows per department (should be uniform)\n",
    "rows_per_dept = df.groupby('dept').size()\n",
    "print(f\"\\nRows per department: min={rows_per_dept.min()}, max={rows_per_dept.max()}, \"\n",
    "      f\"mean={rows_per_dept.mean():.0f}\")\n",
    "if rows_per_dept.min() != rows_per_dept.max():\n",
    "    print(\"âš ï¸  Non-uniform row count â€” some months may be missing for certain departments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1.4d â€” Temporal Coverage Check (no missing months)\n",
    "# ============================================================\n",
    "dates = sorted(df['date_id'].unique())\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  TEMPORAL COVERAGE CHECK\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Date range: {dates[0]} â†’ {dates[-1]}\")\n",
    "print(f\"Unique months: {len(dates)}\")\n",
    "\n",
    "# Check for gaps between consecutive months\n",
    "gaps = []\n",
    "for i in range(1, len(dates)):\n",
    "    prev_y, prev_m = divmod(dates[i-1], 100) if dates[i-1] > 100 else (dates[i-1], 1)\n",
    "    curr_y, curr_m = divmod(dates[i], 100) if dates[i] > 100 else (dates[i], 1)\n",
    "    expected_next = prev_y * 100 + prev_m + 1 if prev_m < 12 else (prev_y + 1) * 100 + 1\n",
    "    if dates[i] != expected_next:\n",
    "        gaps.append((dates[i-1], dates[i]))\n",
    "\n",
    "if not gaps:\n",
    "    print(\"âœ… No temporal gaps â€” continuous month coverage!\")\n",
    "else:\n",
    "    print(f\"ðŸ”´ {len(gaps)} temporal gap(s) detected:\")\n",
    "    for before, after in gaps:\n",
    "        print(f\"   Gap between {before} and {after}\")\n",
    "\n",
    "# Check per-department temporal completeness\n",
    "dept_date_counts = df.groupby('dept')['date_id'].nunique()\n",
    "incomplete_depts = dept_date_counts[dept_date_counts < len(dates)]\n",
    "if len(incomplete_depts) == 0:\n",
    "    print(f\"âœ… All departments have {len(dates)} months (complete)\")\n",
    "else:\n",
    "    print(f\"âš ï¸  {len(incomplete_depts)} department(s) with fewer than {len(dates)} months:\")\n",
    "    for dept, count in incomplete_depts.items():\n",
    "        print(f\"   Dept {dept}: {count}/{len(dates)} months\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1.4e â€” Raw Source Verification (cross-check with source files)\n",
    "# ============================================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "RAW_DIR = Path('../data/raw')\n",
    "PROCESSED_DIR = Path('../data/processed')\n",
    "\n",
    "SOURCES_TO_CHECK = {\n",
    "    'Weather': ('weather/weather_france.csv', 'dept'),\n",
    "    'DPE': ('dpe', None),  # multi-file directory\n",
    "    'INSEE': ('insee', None),\n",
    "    'Eurostat': ('eurostat', None),\n",
    "    'SITADEL': ('sitadel', None),\n",
    "}\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  RAW SOURCE FILE VERIFICATION\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for source_name, (path, dept_col) in SOURCES_TO_CHECK.items():\n",
    "    full_path = RAW_DIR / path\n",
    "    if full_path.is_file():\n",
    "        try:\n",
    "            src_df = pd.read_csv(full_path, dtype={dept_col: str} if dept_col else None)\n",
    "            n_rows = len(src_df)\n",
    "            n_depts = src_df[dept_col].nunique() if dept_col and dept_col in src_df.columns else 'N/A'\n",
    "            status = 'âœ…' if (dept_col is None or n_depts == 96) else f'ðŸ”´ {n_depts}/96 depts'\n",
    "            print(f\"  {source_name:12s} | {str(full_path):45s} | {n_rows:>7,} rows | Depts: {status}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {source_name:12s} | ERROR reading: {e}\")\n",
    "    elif full_path.is_dir():\n",
    "        files = list(full_path.glob('*.csv'))\n",
    "        total_rows = 0\n",
    "        for f in files:\n",
    "            try:\n",
    "                total_rows += len(pd.read_csv(f))\n",
    "            except Exception:\n",
    "                pass\n",
    "        print(f\"  {source_name:12s} | {str(full_path):45s} | {len(files)} CSV files, {total_rows:>7,} total rows\")\n",
    "    else:\n",
    "        print(f\"  {source_name:12s} | âŒ NOT FOUND: {full_path}\")\n",
    "\n",
    "print()\n",
    "print(\"Root cause reference:\")\n",
    "print(\"  - Weather NaN â†’ API rate limiting during Open-Meteo collection (429)\")\n",
    "print(\"  - Lag/rolling NaN â†’ Structural: first N months lack sufficient history\")\n",
    "print(\"  - INSEE/Eurostat NaN â†’ National indicators not yet published for recent months\")\n",
    "print(\"  - SITADEL NaN â†’ Construction permits coverage gap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Missing values analysis\n",
    "\n",
    "NaN values can come from:\n",
    "- Economic indicators not available for some months\n",
    "- Months without DPE in a department\n",
    "- Eurostat indicators not yet published\n",
    "- SITADEL or reference data not available for some department/month combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:18:57.213163Z",
     "iopub.status.busy": "2026-02-16T16:18:57.212807Z",
     "iopub.status.idle": "2026-02-16T16:18:57.222472Z",
     "shell.execute_reply": "2026-02-16T16:18:57.220228Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2.1 â€” Count NaN per column\n",
    "# ============================================================\n",
    "null_pct = (df.isna().mean() * 100).sort_values(ascending=False)\n",
    "null_cols = null_pct[null_pct > 0]\n",
    "\n",
    "if len(null_cols) > 0:\n",
    "    print(f'{len(null_cols)} columns with NaN:')\n",
    "    for col, pct in null_cols.items():\n",
    "        print(f'  {col:40s} : {pct:.1f}% ({df[col].isna().sum()} values)')\n",
    "else:\n",
    "    print('No NaN in the ML-ready dataset!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:18:57.226048Z",
     "iopub.status.busy": "2026-02-16T16:18:57.225725Z",
     "iopub.status.idle": "2026-02-16T16:18:57.919651Z",
     "shell.execute_reply": "2026-02-16T16:18:57.917576Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2.2 â€” NaN heatmap (matrix visualization)\n",
    "# ============================================================\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "sns.heatmap(\n",
    "    df.select_dtypes(include=[np.number]).isna().T,\n",
    "    cbar=False, cmap='Reds', yticklabels=True,\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title('Missing values map (red = NaN)', fontsize=14)\n",
    "ax.set_xlabel('Row index (month x dept)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Distribution of target variables\n",
    "\n",
    "Main target variables:\n",
    "- **nb_installations_pac**: DPE mentioning a heat pump (proxy for heat pump sales)\n",
    "- **nb_installations_clim**: DPE with air conditioning\n",
    "- **nb_dpe_total**: total DPE volume (real estate activity indicator)\n",
    "- **nb_dpe_classe_ab**: DPE class A or B (high-performance buildings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:18:57.924171Z",
     "iopub.status.busy": "2026-02-16T16:18:57.923740Z",
     "iopub.status.idle": "2026-02-16T16:18:58.579008Z",
     "shell.execute_reply": "2026-02-16T16:18:58.577045Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3.1 â€” Histograms of the 4 target variables\n",
    "# ============================================================\n",
    "targets = ['nb_dpe_total', 'nb_installations_pac', \n",
    "           'nb_installations_clim', 'nb_dpe_classe_ab']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Distribution of target variables', fontsize=14)\n",
    "\n",
    "for ax, col in zip(axes.flat, targets):\n",
    "    ax.hist(df[col], bins=30, edgecolor='black', alpha=0.7, density=True, label='Histogram')\n",
    "    df[col].plot(kind='kde', ax=ax, color='red', linewidth=2, label='KDE')\n",
    "    \n",
    "    mean_val = df[col].mean()\n",
    "    median_val = df[col].median()\n",
    "    ax.axvline(mean_val, color='blue', linestyle='--', label=f'Mean={mean_val:.0f}')\n",
    "    ax.axvline(median_val, color='green', linestyle='--', label=f'Median={median_val:.0f}')\n",
    "    \n",
    "    ax.set_title(col)\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:18:58.582534Z",
     "iopub.status.busy": "2026-02-16T16:18:58.582139Z",
     "iopub.status.idle": "2026-02-16T16:18:59.209860Z",
     "shell.execute_reply": "2026-02-16T16:18:59.207420Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3.2 â€” Boxplots of targets by department (top 20 by volume)\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('Target distribution by department (Top 20)', fontsize=14)\n",
    "\n",
    "for ax, col in zip(axes.flat, targets):\n",
    "    # Sort departments by descending median, show top 20\n",
    "    order = df.groupby('dept')[col].median().sort_values(ascending=False).head(20).index\n",
    "    sns.boxplot(data=df[df['dept'].isin(order)], x='dept', y=col, order=order, ax=ax)\n",
    "    ax.set_title(col)\n",
    "    ax.set_xlabel('Department')\n",
    "    ax.tick_params(axis='x', rotation=45, labelsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Time series and trends\n",
    "\n",
    "Examine monthly evolution of target variables to detect:\n",
    "- **Long-term trends** (heat pump market growth?)\n",
    "- **Seasonality** (installation peaks in winter/summer?)\n",
    "- **Anomalies** (unusual months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:18:59.213382Z",
     "iopub.status.busy": "2026-02-16T16:18:59.213128Z",
     "iopub.status.idle": "2026-02-16T16:18:59.227877Z",
     "shell.execute_reply": "2026-02-16T16:18:59.226102Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4.1 â€” Prepare time axis\n",
    "# ============================================================\n",
    "# Convert date_id (YYYYMM) to datetime for plots\n",
    "\n",
    "df['date'] = pd.to_datetime(\n",
    "    df['date_id'].astype(str).str[:4] + '-' + \n",
    "    df['date_id'].astype(str).str[4:] + '-01'\n",
    ")\n",
    "\n",
    "# Aggregate at national level (sum of all departments)\n",
    "df_national = df.groupby('date').agg({\n",
    "    'nb_dpe_total': 'sum',\n",
    "    'nb_installations_pac': 'sum',\n",
    "    'nb_installations_clim': 'sum',\n",
    "    'nb_dpe_classe_ab': 'sum',\n",
    "    'temp_mean': 'mean',\n",
    "    'hdd_sum': 'sum',\n",
    "    'cdd_sum': 'sum',\n",
    "}).reset_index()\n",
    "\n",
    "n_depts = df['dept'].nunique()\n",
    "print(f'National time series: {len(df_national)} months (aggregated from {n_depts} departments)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:18:59.231375Z",
     "iopub.status.busy": "2026-02-16T16:18:59.230874Z",
     "iopub.status.idle": "2026-02-16T16:18:59.511391Z",
     "shell.execute_reply": "2026-02-16T16:18:59.509245Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4.2 â€” Heat pump installations vs temperature (national aggregate)\n",
    "# ============================================================\n",
    "fig, ax1 = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Heat pump installations\n",
    "ax1.plot(df_national['date'], df_national['nb_installations_pac'], \n",
    "         'b-o', markersize=4, label='Heat pump installations', linewidth=2)\n",
    "ax1.fill_between(df_national['date'], df_national['nb_installations_pac'], alpha=0.1, color='blue')\n",
    "ax1.set_ylabel('Nb heat pump installations (national)', color='blue', fontsize=12)\n",
    "\n",
    "# Mean temperature overlay\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(df_national['date'], df_national['temp_mean'], \n",
    "         'r--', alpha=0.7, label='Mean temp.', linewidth=1.5)\n",
    "ax2.set_ylabel('Mean temperature (C)', color='red', fontsize=12)\n",
    "\n",
    "ax1.set_title('Heat pump installations vs Temperature â€” Metropolitan France', fontsize=14)\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:18:59.515135Z",
     "iopub.status.busy": "2026-02-16T16:18:59.514768Z",
     "iopub.status.idle": "2026-02-16T16:19:00.602053Z",
     "shell.execute_reply": "2026-02-16T16:19:00.599858Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4.3 â€” Time series for top 8 departments (by volume)\n",
    "# ============================================================\n",
    "top_depts = df.groupby('dept')['nb_installations_pac'].sum().nlargest(8).index\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 8), sharey=False)\n",
    "fig.suptitle('Heat pump installations â€” Top 8 departments', fontsize=14)\n",
    "\n",
    "for ax, dept in zip(axes.flat, top_depts):\n",
    "    dept_data = df[df['dept'] == dept].sort_values('date')\n",
    "    dept_name = dept_data['dept_name'].iloc[0] if 'dept_name' in dept_data.columns else dept\n",
    "    \n",
    "    ax.plot(dept_data['date'], dept_data['nb_installations_pac'], \n",
    "            '-o', markersize=2, linewidth=1)\n",
    "    ax.set_title(f'{dept_name} ({dept})', fontsize=10)\n",
    "    ax.tick_params(axis='x', rotation=45, labelsize=7)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. HVAC market seasonality\n",
    "\n",
    "Business hypotheses:\n",
    "- **Heating (heat pumps)**: installation peak in autumn (winter anticipation)\n",
    "- **Air conditioning**: peak in spring/early summer (before heatwaves)\n",
    "- **Total DPE**: linked to real estate transactions (spring peak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:19:00.606795Z",
     "iopub.status.busy": "2026-02-16T16:19:00.606390Z",
     "iopub.status.idle": "2026-02-16T16:19:01.376808Z",
     "shell.execute_reply": "2026-02-16T16:19:01.375181Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5.1 â€” Seasonal boxplots (by month)\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Seasonality â€” Distribution by month', fontsize=14)\n",
    "\n",
    "month_labels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "for ax, col in zip(axes.flat, targets):\n",
    "    sns.boxplot(data=df, x='month', y=col, ax=ax)\n",
    "    ax.set_xticklabels(month_labels, rotation=45)\n",
    "    ax.set_title(col)\n",
    "    ax.set_xlabel('')\n",
    "    \n",
    "    # Color the seasons\n",
    "    for i in range(12):\n",
    "        if i+1 in [10, 11, 12, 1, 2, 3]:  # Heating season\n",
    "            ax.axvspan(i-0.5, i+0.5, alpha=0.05, color='blue')\n",
    "        elif i+1 in [6, 7, 8, 9]:  # Cooling season\n",
    "            ax.axvspan(i-0.5, i+0.5, alpha=0.05, color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:19:01.380556Z",
     "iopub.status.busy": "2026-02-16T16:19:01.380283Z",
     "iopub.status.idle": "2026-02-16T16:19:01.738672Z",
     "shell.execute_reply": "2026-02-16T16:19:01.737012Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5.2 â€” Heatmap month x department (heat pump installations, top 15)\n",
    "# ============================================================\n",
    "top15_depts = df.groupby('dept_name' if 'dept_name' in df.columns else 'dept')[\n",
    "    'nb_installations_pac'\n",
    "].mean().nlargest(15).index\n",
    "\n",
    "col_name = 'dept_name' if 'dept_name' in df.columns else 'dept'\n",
    "df_top = df[df[col_name].isin(top15_depts)]\n",
    "\n",
    "pivot_pac = df_top.pivot_table(\n",
    "    values='nb_installations_pac', \n",
    "    index='month', \n",
    "    columns=col_name,\n",
    "    aggfunc='mean'\n",
    ")\n",
    "pivot_pac.index = month_labels\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "sns.heatmap(\n",
    "    pivot_pac, annot=True, fmt='.0f', cmap='YlOrRd',\n",
    "    linewidths=0.5, ax=ax\n",
    ")\n",
    "ax.set_title('Mean heat pump installations â€” Month x Department (Top 15)', fontsize=14)\n",
    "ax.set_xlabel('Department')\n",
    "ax.set_ylabel('Month')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Department comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:19:01.742344Z",
     "iopub.status.busy": "2026-02-16T16:19:01.741865Z",
     "iopub.status.idle": "2026-02-16T16:19:01.756442Z",
     "shell.execute_reply": "2026-02-16T16:19:01.754807Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6.1 â€” Mean volume per department (top 20)\n",
    "# ============================================================\n",
    "col_name = 'dept_name' if 'dept_name' in df.columns else 'dept'\n",
    "dept_stats = df.groupby(col_name).agg({\n",
    "    'nb_dpe_total': 'mean',\n",
    "    'nb_installations_pac': 'mean',\n",
    "    'nb_installations_clim': 'mean',\n",
    "    'pct_pac': 'mean',\n",
    "}).round(1).sort_values('nb_dpe_total', ascending=False)\n",
    "\n",
    "print(f'Mean monthly statistics by department (top 20 / {len(dept_stats)} total):')\n",
    "dept_stats.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:19:01.759980Z",
     "iopub.status.busy": "2026-02-16T16:19:01.759693Z",
     "iopub.status.idle": "2026-02-16T16:19:02.090565Z",
     "shell.execute_reply": "2026-02-16T16:19:02.088740Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6.2 â€” Comparative bar charts (top 15 departments)\n",
    "# ============================================================\n",
    "top15_stats = dept_stats.head(15)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 6))\n",
    "fig.suptitle('Department comparison â€” Monthly mean (Top 15)', fontsize=14)\n",
    "\n",
    "for ax, col, title in zip(axes, \n",
    "    ['nb_dpe_total', 'nb_installations_pac', 'pct_pac'],\n",
    "    ['Total DPE volume', 'Heat pump installations', 'Heat pump rate (%)']):\n",
    "    \n",
    "    top15_stats[col].plot(kind='barh', ax=ax, color='steelblue')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Correlations between variables\n",
    "\n",
    "Objective: identify the features most correlated with targets\n",
    "to guide feature selection for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:19:02.094680Z",
     "iopub.status.busy": "2026-02-16T16:19:02.094386Z",
     "iopub.status.idle": "2026-02-16T16:19:02.651569Z",
     "shell.execute_reply": "2026-02-16T16:19:02.649946Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7.1 â€” Correlation matrix (base features)\n",
    "# ============================================================\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "exclude = ['date_id', 'latitude', 'longitude']\n",
    "corr_cols = [c for c in numeric_cols if c not in exclude]\n",
    "\n",
    "corr_matrix = df[corr_cols].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 14))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "sns.heatmap(\n",
    "    corr_matrix, mask=mask, annot=False, cmap='RdBu_r',\n",
    "    center=0, vmin=-1, vmax=1, linewidths=0.5,\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title('Correlation matrix â€” Base features', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:19:02.656170Z",
     "iopub.status.busy": "2026-02-16T16:19:02.655784Z",
     "iopub.status.idle": "2026-02-16T16:19:02.664586Z",
     "shell.execute_reply": "2026-02-16T16:19:02.662974Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7.2 â€” Top correlations with target (nb_installations_pac)\n",
    "# ============================================================\n",
    "target = 'nb_installations_pac'\n",
    "\n",
    "corr_target = corr_matrix[target].drop(target).abs().sort_values(ascending=False)\n",
    "\n",
    "print(f'Top 15 correlations with {target}:')\n",
    "print('-' * 50)\n",
    "for feat, corr_val in corr_target.head(15).items():\n",
    "    sign = '+' if corr_matrix.loc[feat, target] > 0 else '-'\n",
    "    print(f'  {feat:40s} : {sign}{corr_val:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T16:19:02.669091Z",
     "iopub.status.busy": "2026-02-16T16:19:02.668247Z",
     "iopub.status.idle": "2026-02-16T16:19:03.337812Z",
     "shell.execute_reply": "2026-02-16T16:19:03.335224Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7.3 â€” Scatter plots: key features vs target\n",
    "# ============================================================\n",
    "key_features = ['nb_dpe_total', 'temp_mean', 'hdd_sum', \n",
    "                'confiance_menages', 'ipi_hvac_c28', 'cdd_sum']\n",
    "key_features = [f for f in key_features if f in df.columns]\n",
    "\n",
    "n = len(key_features)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "fig.suptitle(f'Feature relationships -> {target}', fontsize=14)\n",
    "\n",
    "for ax, feat in zip(axes.flat, key_features):\n",
    "    ax.scatter(df[feat], df[target], alpha=0.3, s=10)\n",
    "    \n",
    "    # Trend line\n",
    "    mask = df[[feat, target]].dropna().index\n",
    "    if len(mask) > 5:\n",
    "        z = np.polyfit(df.loc[mask, feat], df.loc[mask, target], 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_range = np.linspace(df[feat].min(), df[feat].max(), 100)\n",
    "        ax.plot(x_range, p(x_range), 'r-', linewidth=2, alpha=0.7)\n",
    "    \n",
    "    r = df[feat].corr(df[target])\n",
    "    ax.set_title(f'{feat}\\n(r = {r:.3f})', fontsize=10)\n",
    "    ax.set_xlabel(feat)\n",
    "    ax.set_ylabel(target)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "for i in range(n, len(axes.flat)):\n",
    "    axes.flat[i].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Conclusions for modeling\n",
    "\n",
    "### Observed data characteristics:\n",
    "- **Dataset**: ~5376 rows (96 departments x 56 months), 60+ features\n",
    "- **Zero NaN**: the merge pipeline produces a clean, complete dataset â€” no imputation bias\n",
    "- **Target** (`nb_installations_pac`): right-skewed distribution with high inter-department variance (Ile-de-France departments >> rural areas)\n",
    "\n",
    "### Key observations:\n",
    "\n",
    "1. **Target distributions**: volumes vary greatly between departments (Paris/Ile-de-France >> rural departments). Mean ~107 installations/month/dept but with std ~100+ indicating huge variability. The model must capture this geographic effect.\n",
    "\n",
    "2. **Seasonality**: visible seasonal pattern â€” heat pump installations peak in autumn (Oct-Nov, anticipation of winter) and dip in summer (Aug). Air conditioning peaks in spring/early summer (before heatwaves). Weather features (HDD/CDD) will be essential.\n",
    "\n",
    "3. **Trend**: heat pump market shows growth over the 2021-2025 period, driven by MaPrimeRenov' incentives and energy transition policies. The `year` feature captures this dynamic.\n",
    "\n",
    "4. **Correlations**: `nb_dpe_total` is strongly correlated with `nb_installations_pac` (volume effect: more DPE = more heat pump detections). `hdd_sum` shows positive correlation (cold weather -> more heating installations). Economic indicators (household confidence, business climate) add supplementary signal.\n",
    "\n",
    "5. **New features**: SITADEL (construction permits) and INSEE reference (altitude, population density) provide department-level structural context. These features are integrated via `python -m src.pipeline merge` then `features`.\n",
    "\n",
    "### Implications for ML:\n",
    "- **Ridge Regression**: strong baseline candidate â€” L2 regularization handles correlated features (lags, rolling) well\n",
    "- **LightGBM**: can capture non-linear interactions (weather x department, season x economic indicators)\n",
    "- **Prophet**: adapted to the time series component with external regressors, but limited by small data per department (~36 months each)\n",
    "- **LSTM**: exploratory only â€” insufficient data volume for deep learning\n",
    "\n",
    "### Improvement suggestions:\n",
    "- **Stationarity test** (ADF / KPSS) on national time series â€” confirm whether differencing is needed\n",
    "- **Granger causality test** for economic indicators â€” quantify predictive lead of confidence indices\n",
    "- **Violin plots** instead of boxplots for richer distribution view (shows multimodality)\n",
    "- **Cross-correlation analysis** at different lags between weather and installations (optimal lag identification)\n",
    "\n",
    "### Next steps:\n",
    "- Notebook 02: ML modeling (Ridge, LightGBM, Prophet)\n",
    "- Notebook 05: Feature review and SHAP analysis (column-level keep/drop decisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Summary\n",
    "# ============================================================\n",
    "n_depts = df['dept'].nunique()\n",
    "print('=' * 60)\n",
    "print('EDA SUMMARY')\n",
    "print('=' * 60)\n",
    "print(f'Dataset: {df.shape[0]} rows x {df.shape[1]} columns')\n",
    "print(f'Period: {df[\"date_id\"].min()} -> {df[\"date_id\"].max()}')\n",
    "print(f'Departments: {n_depts}')\n",
    "print(f'Total NaN: {df.isna().sum().sum()} ({(df.isna().mean().mean()*100):.1f}%)')\n",
    "print(f'\\nTarget variable (nb_installations_pac):')\n",
    "print(f'  Mean: {df[\"nb_installations_pac\"].mean():.1f}')\n",
    "print(f'  Std: {df[\"nb_installations_pac\"].std():.1f}')\n",
    "print(f'  Min: {df[\"nb_installations_pac\"].min()}')\n",
    "print(f'  Max: {df[\"nb_installations_pac\"].max()}')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8 â€” New features: SITADEL & INSEE Filosofi reference\n",
    "# ============================================================\n",
    "# Check which new features are available in the dataset\n",
    "\n",
    "# SITADEL (construction permits, per department)\n",
    "sitadel_cols = [c for c in df.columns if c.startswith('nb_logements') or c.startswith('surface_autorisee')]\n",
    "\n",
    "# INSEE Filosofi reference (static, per department)\n",
    "ref_cols = [c for c in df.columns if c in [\n",
    "    'revenu_median', 'prix_m2_median', 'nb_logements_total', 'pct_maisons'\n",
    "]]\n",
    "\n",
    "new_features = sitadel_cols + ref_cols\n",
    "\n",
    "if new_features:\n",
    "    print(f'New features found: {len(new_features)}')\n",
    "    for feat in new_features:\n",
    "        null_pct = df[feat].isna().mean() * 100\n",
    "        r = df[feat].corr(df[target]) if df[feat].notna().sum() > 10 else float('nan')\n",
    "        print(f'  {feat:35s} : NaN={null_pct:.1f}%, corr={r:+.3f}')\n",
    "    \n",
    "    # Scatter plots for reference features\n",
    "    plot_feats = [f for f in new_features if f in df.columns and df[f].notna().sum() > 10]\n",
    "    if plot_feats:\n",
    "        n = min(len(plot_feats), 4)\n",
    "        fig, axes = plt.subplots(1, n, figsize=(4 * n, 4))\n",
    "        if n == 1:\n",
    "            axes = [axes]\n",
    "        fig.suptitle(f'New features vs {target}', fontsize=13)\n",
    "        for ax, feat in zip(axes, plot_feats[:4]):\n",
    "            ax.scatter(df[feat], df[target], alpha=0.3, s=10)\n",
    "            r = df[feat].corr(df[target])\n",
    "            ax.set_title(f'{feat}\\n(r={r:+.3f})', fontsize=9)\n",
    "            ax.set_xlabel(feat, fontsize=8)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print('No SITADEL or reference features found in the ML dataset.')\n",
    "    print('These features are in hvac_features_dataset.csv (used by modeling notebooks).')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

"""
Shared dependencies for the HVAC API.

Loads ML models, scaler, and imputer once at startup,
and exposes the objects via an AppState singleton accessible by all endpoints.
"""

from __future__ import annotations

import os
import pickle
import time
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any

import numpy as np
import pandas as pd

# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

API_VERSION: str = "1.0.0"
BASE_DIR: Path = Path(__file__).resolve().parent.parent
DATA_DIR: Path = BASE_DIR / "data"
MODELS_DIR: Path = DATA_DIR / "models"
FEATURES_DIR: Path = DATA_DIR / "features"
RAW_DIR: Path = DATA_DIR / "raw"

FEATURES_CSV: Path = FEATURES_DIR / "hvac_features_dataset.csv"
RIDGE_PKL: Path = MODELS_DIR / "ridge_model.pkl"
LGB_PKL: Path = MODELS_DIR / "lightgbm_model.pkl"
SCALER_PKL: Path = MODELS_DIR / "scaler.pkl"
IMPUTER_PKL: Path = MODELS_DIR / "imputer.pkl"
TRAINING_RESULTS_CSV: Path = MODELS_DIR / "training_results.csv"
EVALUATION_REPORT: Path = MODELS_DIR / "evaluation_report.txt"

TARGET_COL: str = "nb_installations_pac"

# Non-feature columns (to exclude before prediction)
NON_FEATURE_COLS: set[str] = {
    "date_id", "dept", "nb_dpe_total", TARGET_COL,
    "nb_installations_clim", "nb_dpe_classe_ab",
    "dept_name", "city_ref", "latitude", "longitude",
    "n_valid_features", "pct_valid_features",
    "_outlier_iqr", "_outlier_zscore", "_outlier_iforest",
    "_outlier_consensus", "_outlier_score",
    # Derived from targets — data leakage if included as features
    "pct_pac", "pct_clim", "pct_classe_ab",
}

# Single source of truth for department names (imported from config)
from config.settings import DEPT_NAMES
DEPARTEMENTS: dict[str, str] = DEPT_NAMES


# ---------------------------------------------------------------------------
# Global application state (singleton)
# ---------------------------------------------------------------------------

@dataclass
class AppState:
    """Global state loaded once at API startup."""

    ridge_model: Any = None
    lgb_model: Any = None
    scaler: Any = None
    imputer: Any = None
    features_df: pd.DataFrame | None = None
    feature_names: list[str] = field(default_factory=list)
    training_results: pd.DataFrame | None = None
    start_time: float = field(default_factory=time.time)
    model_date: str | None = None

    # -- loading ------------------------------------------------------------

    def load(self) -> None:
        """Load all artifacts from disk."""
        self._load_model()
        self._load_features()
        self._load_training_results()

    def _load_model(self) -> None:
        """Load the Ridge/LightGBM models, scaler, and imputer.

        Security note: pickle.load is used here on LOCAL files only
        (generated by our own training pipeline). Never load untrusted
        pickle files — consider joblib or ONNX for production.
        """
        for pkl_path in (RIDGE_PKL, SCALER_PKL, IMPUTER_PKL):
            if not pkl_path.exists():
                raise FileNotFoundError(f"Model file not found: {pkl_path}")
        with open(RIDGE_PKL, "rb") as f:
            self.ridge_model = pickle.load(f)  # noqa: S301 — trusted local file
        with open(SCALER_PKL, "rb") as f:
            self.scaler = pickle.load(f)  # noqa: S301
        with open(IMPUTER_PKL, "rb") as f:
            self.imputer = pickle.load(f)  # noqa: S301

        # LightGBM (optional — best model but larger file)
        if LGB_PKL.exists():
            with open(LGB_PKL, "rb") as f:
                self.lgb_model = pickle.load(f)  # noqa: S301

        self.feature_names = list(self.ridge_model.feature_names_in_)
        # Last modification date of the model file
        mtime = os.path.getmtime(RIDGE_PKL)
        self.model_date = datetime.fromtimestamp(mtime).strftime("%Y-%m-%d")

    def _load_features(self) -> None:
        """Load the features dataset."""
        self.features_df = pd.read_csv(FEATURES_CSV, dtype={"dept": str})

    def _load_training_results(self) -> None:
        """Load the training results."""
        if TRAINING_RESULTS_CSV.exists():
            self.training_results = pd.read_csv(TRAINING_RESULTS_CSV)

    # -- prediction ---------------------------------------------------------

    def predict(
        self,
        dept: str,
        horizon: int = 1,
        model_name: str = "lightgbm",
        extra_features: dict[str, float] | None = None,
    ) -> list[dict[str, Any]]:
        """
        Generate predictions for a department over N months.

        Uses the last known row for the department as the starting point,
        then projects iteratively by re-injecting the previous prediction
        into the lag features.

        Args:
            dept: Department code (e.g., "69").
            horizon: Number of months to predict (1-24).
            model_name: Model to use ("ridge" or "lightgbm").
            extra_features: Optional feature overrides.

        Returns a list of dicts {date, predicted_value, lower_bound, upper_bound}.
        """
        df_dept = self.features_df[self.features_df["dept"] == dept].copy()
        if df_dept.empty:
            return []

        df_dept = df_dept.sort_values("date_id")
        last_row = df_dept.iloc[-1].copy()
        last_date_str = str(int(last_row["date_id"]))

        # Select model
        use_lgb = model_name == "lightgbm" and self.lgb_model is not None
        rmse = self._get_model_rmse(model_name)

        results: list[dict[str, Any]] = []
        current_row = last_row.copy()

        for step in range(1, horizon + 1):
            # Advance the date by one month
            next_date = self._next_month(last_date_str, step)

            # Update temporal features
            current_row = self._advance_features(current_row, next_date)

            # Inject additional features
            if extra_features:
                for feat, val in extra_features.items():
                    if feat in current_row.index:
                        current_row[feat] = val

            # Extract features in the correct order
            X = current_row[self.feature_names].values.reshape(1, -1).astype(float)
            X = self.imputer.transform(X)

            if use_lgb:
                # LightGBM uses imputed data (no scaling needed)
                pred = float(self.lgb_model.predict(X)[0])
            else:
                # Ridge uses scaled data
                X_scaled = self.scaler.transform(X)
                pred = float(self.ridge_model.predict(X_scaled)[0])

            pred = max(pred, 0.0)  # No negative counts

            # ~95% confidence interval (1.96 * RMSE)
            margin = 1.96 * rmse
            results.append({
                "date": f"{next_date[:4]}-{next_date[4:]}",
                "predicted_value": round(pred, 2),
                "lower_bound": round(max(pred - margin, 0), 2),
                "upper_bound": round(pred + margin, 2),
            })

            # Re-inject the prediction into lags for the next step
            current_row["nb_installations_pac_lag_1m"] = pred

        return results

    def _get_model_rmse(self, model_name: str = "ridge") -> float:
        """Retrieve the test RMSE for a given model from the results."""
        if self.training_results is not None:
            model_row = self.training_results[
                self.training_results["model"] == model_name
            ]
            if not model_row.empty:
                val = model_row.iloc[0].get("test_rmse", np.nan)
                if not np.isnan(val):
                    return float(val)
        return 1.0  # fallback

    @staticmethod
    def _next_month(base_yyyymm: str, offset: int) -> str:
        """Calculate the YYYYMM date after 'offset' months."""
        year = int(base_yyyymm[:4])
        month = int(base_yyyymm[4:])
        total = (year * 12 + month - 1) + offset
        new_year = total // 12
        new_month = total % 12 + 1
        return f"{new_year}{new_month:02d}"

    @staticmethod
    def _advance_features(row: pd.Series, new_date: str) -> pd.Series:
        """Update temporal columns for the new date."""
        row = row.copy()
        year = int(new_date[:4])
        month = int(new_date[4:])
        row["year"] = year
        row["month"] = month
        row["quarter"] = (month - 1) // 3 + 1
        row["is_heating"] = int(month in (1, 2, 3, 10, 11, 12))
        row["is_cooling"] = int(month in (6, 7, 8))
        row["month_sin"] = round(np.sin(2 * np.pi * month / 12), 3)
        row["month_cos"] = round(np.cos(2 * np.pi * month / 12), 3)
        row["year_trend"] = (year - 2020) + month / 12
        return row


# Global instance
state = AppState()
